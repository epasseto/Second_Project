{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Testing\n",
    "\n",
    "---\n",
    "\n",
    "**Run on terminal** requisites:\n",
    "\n",
    "\"If someone in the future comes with a revised or new dataset of messages, they should be able to easily create a new model just by running your code. These Python scripts should be able to run with additional arguments specifying the files used for the data and model.\"\n",
    "\n",
    "`python process_data.py disaster_messages.csv disaster_categories.csv DisasterResponse.db`\n",
    "\n",
    "`python train_classifier.py ../data/DisasterResponse.db classifier.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the test part of Machine Learning Pipeline\n",
    "\n",
    ">- import libraries\n",
    ">- read data from a SQLite table named `sqlite:///Messages.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from time import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "import udacourse2 #my library for this project\n",
    "import train_classifier as tr #my pipeline\n",
    "\n",
    "data_file = 'sqlite:///Messages.db' #sys.argv[1] \n",
    "classifier = 'classifier.pkl' #sys.argv[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test `load_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tr.load_data(data_file=data_file,\n",
    "                    remove_cols=True,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test `build_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = tr.build_model(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tr.train(X=X,\n",
    "                 y=y,\n",
    "                 model=model_pipeline,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test `export_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'classifier.pkl'\n",
    "\n",
    "tr.export_model(model=model,\n",
    "                file_name=file_name,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    model_unpk = pickle.load(pk_reader)\n",
    "    \n",
    "model_unpk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test `run_pipeline` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'sqlite:///Messages.db'\n",
    "start = time()\n",
    "\n",
    "tr.run_pipeline(data_file=data_file, verbose=False)\n",
    "\n",
    "spent = time() - start\n",
    "print('process time: {:.0f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.`Main` function calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval(pp.pformat(tr.main.__doc__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.run_pipeline(data_file=data_file,\n",
    "                classifier=classifier,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Area\n",
    "\n",
    "SQL correct string for URL [here](https://stackoverflow.com/questions/49776619/sqlalchemy-exc-argumenterror-could-not-parse-rfc1738-url-from-string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.main(data_file=data_file,\n",
    "        classifier=classifier,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = sys.argv\n",
    "\n",
    "simul_args = ['xuru.db', 'boco.pkl', '-r', '-C=3.', '-t=.2', '-a', '-v']\n",
    "#simul_args = ['xuru.db']\n",
    "Esimul_args = ['xuru.db', 'boco.pkl', '-v', '-a', '-C=4.0', '-r', '-xu']\n",
    "optionals = ['-r', '-C', '-t', '-a', '-v']\n",
    "args = simul_args\n",
    "\n",
    "#first, set default arguments\n",
    "data_file = '../data/DisasterResponse.db'\n",
    "classifier = 'classifier.pkl'\n",
    "remove_cols = False\n",
    "C = 2.0\n",
    "test_size = 0.25\n",
    "best_10 = True\n",
    "verbose = False\n",
    "\n",
    "#second, try to change the two main arguments\n",
    "try:\n",
    "    args[0]\n",
    "except IndexError:\n",
    "    pass\n",
    "else:\n",
    "    data_file = args[0]   \n",
    "try:\n",
    "    args[1]\n",
    "except IndexError:\n",
    "    pass\n",
    "else:\n",
    "    classifier = args[1]\n",
    "\n",
    "remain_args = args[2:] #elliminate the two main arg    \n",
    "if len(remain_args) > 0:\n",
    "    for arg in remain_args:\n",
    "        comm = arg[:2] #get the command part\n",
    "        if comm == '-r':\n",
    "            remove_cols = True\n",
    "        elif comm == '-C':\n",
    "            C = arg[3:]\n",
    "        elif comm == '-t':\n",
    "            test_size = arg[3:]\n",
    "        elif comm == '-a':\n",
    "            best_10=False\n",
    "        elif comm == '-v':\n",
    "            verbose=True\n",
    "        else:\n",
    "            raise Exception('invalid argument')\n",
    "\n",
    "print('data_file={} classifier={} remove_cols={} C={} test_size={} best_10={} verbose={}'\\\n",
    "      .format(data_file, classifier, remove_cols, C, test_size, best_10, verbose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg='-s;'\n",
    "\n",
    "arg[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
