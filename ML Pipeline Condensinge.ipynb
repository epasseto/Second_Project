{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Condensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#measuring time and making basic math\n",
    "from time import time\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "#import statistics\n",
    "\n",
    "#my own ETL pipeline\n",
    "#import process_data as pr\n",
    "\n",
    "#dealing with datasets and showing content\n",
    "import pandas as pd\n",
    "#import pprint as pp\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#natural language toolkit\n",
    "#from nltk.tokenize import word_tokenize \n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#REGEX toolkit\n",
    "#import re\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.ensemble import RandomForestClassifier #need MOClassifier!\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Machine Learning Metrics\n",
    "#from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#pickling tool\n",
    "import pickle\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing MySQL to Pandas - load data from database\n",
    "engine = create_engine('sqlite:///Messages.db', poolclass=pool.NullPool) #, echo=True)\n",
    "\n",
    "#retrieving tables names from my DB\n",
    "#https://stackoverflow.com/questions/6473925/sqlalchemy-getting-a-list-of-tables\n",
    "inspector = inspect(engine)\n",
    "if verbose:\n",
    "    print('existing tables in my SQLite database:', inspector.get_table_names())\n",
    "\n",
    "connection = engine.connect()\n",
    "df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "connection.close()\n",
    "df.name = 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I.Prepare the data\n",
    "if verbose:\n",
    "    print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['if_blank'] == 0]\n",
    "if verbose:\n",
    "    print('remaining rows:', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying if removal was complete\n",
    "if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "    if verbose:\n",
    "        print('removal complete!')\n",
    "else:\n",
    "    raise Exception('something went wrong with rows removal before training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Premature Tokenization Strategy (pre-tokenizer)\n",
    "#Pre-Tokenizer + not removing provisory tokenized column\n",
    "start = time()\n",
    "\n",
    "#inserting a tokenized column\n",
    "try:\n",
    "    df = df.drop('tokenized', axis=1)\n",
    "except KeyError:\n",
    "    print('OK')\n",
    "df.insert(1, 'tokenized', np.nan)\n",
    "\n",
    "#tokenizing over the provisory\n",
    "df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "\n",
    "#removing NaN over provisory (if istill exist)\n",
    "df = df[df['tokenized'].notnull()]\n",
    "\n",
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "if verbose:\n",
    "    print('found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "if verbose:\n",
    "    print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "#I will drop 'message' column\n",
    "try:\n",
    "    df = df.drop('message', axis=1)\n",
    "except KeyError:\n",
    "    if verbose:\n",
    "        print('OK')\n",
    "\n",
    "if verbose:\n",
    "    print('now I have {} rows to train'.format(df.shape[0]))\n",
    "\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('process time:{:.0f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database Data Consistency Fix\n",
    "start = time()\n",
    "\n",
    "#correction for aid_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='aid',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for weather_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='wtr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for infrastrucutre_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='ifr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for related(considering that the earlier were already corrected)\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='main',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('Data Consistency performed')\n",
    "    print('process time:{:.0f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#II.Break the data\n",
    "#X is the Training Text Column\n",
    "X = df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is the Classification labels\n",
    "#I REMOVED \"related\" column from my labels, as it is impossible to train it!\n",
    "y = df[df.columns[4:]]\n",
    "#y = df[df.columns[5:]]\n",
    "\n",
    "#remove_lst = []\n",
    "\n",
    "#for column in y.columns:\n",
    "#    col = y[column]\n",
    "#    if (col == 0).all():\n",
    "#        if verbose:\n",
    "#            print('*{} -> only zeroes training column!'.format(column))\n",
    "#        remove_lst.append(column)\n",
    "#    else:\n",
    "        #print('*{} -> column OK'.format(column))\n",
    "#        pass\n",
    "\n",
    "#if verbose:\n",
    "#    print(remove_lst)\n",
    "\n",
    "#y = y.drop(remove_lst, axis=1)\n",
    "\n",
    "if y.shape[1] == 36:\n",
    "    if verbose:\n",
    "        print('y dataset has 36 labels')\n",
    "else:\n",
    "    raise Exception('something went wrong, dataset has {} labels instead of 36'.format(y.shape[1]))\n",
    "\n",
    "if verbose:\n",
    "    print('Dataset breaked into X-Training Text Column and Y-Multilabels for Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#III.Slit the data\n",
    "#Split makes randomization, so random_state parameter was set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (X_train.shape[0] + X_test.shape[0]) == X.shape[0]:\n",
    "    if verbose:\n",
    "        print('data split into train and text seems OK')\n",
    "else:\n",
    "    raise Exception('something went wrong when splitting the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV. Train your Classifier\n",
    "#Classifier is Support Vector Machine\n",
    "if verbose:\n",
    "    print('Classifier training started')\n",
    "\n",
    "start = time()\n",
    "    \n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "feats = TfidfVectorizer(analyzer='word', \n",
    "                        tokenizer=dummy, \n",
    "                        preprocessor=dummy,\n",
    "                        token_pattern=None,\n",
    "                        ngram_range=(1, 3))\n",
    "\n",
    "classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                        random_state=42))\n",
    "\n",
    "pipeline_lnsv = Pipeline([('vect', feats),\n",
    "                          ('clf', classif)])\n",
    "\n",
    "pipeline_lnsv.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('LINEAR SUPPORT VECTOR MACHINE - process time:{:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_lnsv.predict(X_test)\n",
    "if verbose:\n",
    "    metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                 y_pred,\n",
    "                                 best_10=True,\n",
    "                                 verbose=True)\n",
    "else:\n",
    "    metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                           y_pred,\n",
    "                                           best_10=True,\n",
    "                                           verbose=False)\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric < 0.6:\n",
    "        raise Exception('something is wrong, model is predicting poorly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_adab.fit(X_train, y_train)\n",
    "\n",
    "file_name = 'classifier.pkl'\n",
    "\n",
    "#writing the file\n",
    "with open (file_name, 'wb') as pk_writer: \n",
    "    pickle.dump(pipeline_lnsv, pk_writer)\n",
    "\n",
    "#reading the file\n",
    "with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    pipeline_lnsv = pickle.load(pk_reader)\n",
    "    \n",
    "#pipeline_lnsv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the notebook to complete `train.py`\n",
    "\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pickling tool\n",
    "import pickle\n",
    "\n",
    "#only a dummy function, as I pre-tokenize my data\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def load_data(data_file, \n",
    "              verbose=False):\n",
    "    '''This function takes a path for a SQLite table and returns processed data\n",
    "    for training a Machine Learning Classifier\n",
    "    Inputs:\n",
    "      - data_file (mandatory) - full path for SQLite table - text string\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Oputputs:\n",
    "      - X - tokenized text X-training - Pandas Series\n",
    "      - y - y-multilabels 0|1 - Pandas Dataframe'''\n",
    "    if verbose:\n",
    "        print('###load_data function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.read in file\n",
    "    #importing MySQL to Pandas - load data from database\n",
    "    engine = create_engine(data_file, poolclass=pool.NullPool) #, echo=True)\n",
    "    #retrieving tables names from my DB\n",
    "    inspector = inspect(engine)\n",
    "    if verbose:\n",
    "        print('existing tables in my SQLite database:', inspector.get_table_names())\n",
    "    connection = engine.connect()\n",
    "    df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "    connection.close()\n",
    "    df.name = 'df'\n",
    "    \n",
    "    #2.clean data\n",
    "    #2.1.Elliminate rows with all-blank labels\n",
    "    if verbose:\n",
    "        print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))\n",
    "    df = df[df['if_blank'] == 0]\n",
    "    if verbose:\n",
    "        print('remaining rows:', df.shape[0])\n",
    "    #Verifying if removal was complete\n",
    "    if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "        if verbose:\n",
    "            print('removal complete!')\n",
    "        else:\n",
    "            raise Exception('something went wrong with rows removal before training')\n",
    "            \n",
    "    #2.2.Premature Tokenization Strategy (pre-tokenizer)\n",
    "    #Pre-Tokenizer + not removing provisory tokenized column\n",
    "    #inserting a tokenized column\n",
    "    try:\n",
    "        df = df.drop('tokenized', axis=1)\n",
    "    except KeyError:\n",
    "        print('OK')\n",
    "    df.insert(1, 'tokenized', np.nan)\n",
    "    #tokenizing over the provisory\n",
    "    df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "    #removing NaN over provisory (if istill exist)\n",
    "    df = df[df['tokenized'].notnull()]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('found {} rows with no tokens'.format(empty_tokens))\n",
    "    df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "    #I will drop the original 'message' column\n",
    "    try:\n",
    "        df = df.drop('message', axis=1)\n",
    "    except KeyError:\n",
    "        if verbose:\n",
    "            print('OK')\n",
    "    if verbose:\n",
    "        print('now I have {} rows to train'.format(df.shape[0]))\n",
    "\n",
    "    #2.3.Database Data Consistency Check/Fix\n",
    "    #correction for aid_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='aid',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for weather_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='wtr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for infrastrucutre_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='ifr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for related(considering that the earlier were already corrected)\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='main',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    \n",
    "    #load to database <-I don't know for what it is\n",
    "    \n",
    "    #3.Define features and label arrays (break the data)\n",
    "    #3.1.X is the Training Text Column\n",
    "    X = df['tokenized']\n",
    "    #3.2.y is the Classification labels\n",
    "    #I REMOVED \"related\" column from my labels, as it is impossible to train it!\n",
    "    y = df[df.columns[5:]]\n",
    "    remove_lst = []\n",
    "\n",
    "    for column in y.columns:\n",
    "        col = y[column]\n",
    "        if (col == 0).all():\n",
    "            if verbose:\n",
    "                print('*{} -> only zeroes training column!'.format(column))\n",
    "            remove_lst.append(column)\n",
    "        else:\n",
    "            #print('*{} -> column OK'.format(column))\n",
    "            pass\n",
    "        \n",
    "    if verbose:\n",
    "        print(remove_lst)\n",
    "    y = y.drop(remove_lst, axis=1)\n",
    "    \n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*dataset breaked into X-Training Text Column and Y-Multilabels')    \n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def build_model(verbose=False):\n",
    "    '''This function builds the Classifier Pipeline, for future fitting\n",
    "    Inputs:\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - model_pipeline for your Classifiear (untrained)\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###build_model function started')\n",
    "    start = time()\n",
    "    \n",
    "    #1.text processing and model pipeline\n",
    "    #(text processing was made at a earlier step, at Load Data function)\n",
    "    feats = TfidfVectorizer(analyzer='word', \n",
    "                            tokenizer=dummy, \n",
    "                            preprocessor=dummy,\n",
    "                            token_pattern=None,\n",
    "                            ngram_range=(1, 3))\n",
    "    \n",
    "    classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                            random_state=42))\n",
    "    \n",
    "    model_pipeline = Pipeline([('vect', feats),\n",
    "                               ('clf', classif)])\n",
    "    \n",
    "    #define parameters for GridSearchCV (parameters already defined)\n",
    "    #create gridsearch object and return as final model pipeline (made at pipeline preparation)\n",
    "    #obs: for better performance, I pre-tokenized my data. And GridSearch was runned on Jupyter,\n",
    "    #     and the best parameters where adjusted, just to save processing time during code execution.\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*Linear Support Vector Machine pipeline was created')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def train(X, \n",
    "          y, \n",
    "          model, \n",
    "          verbose=False):\n",
    "    '''This function trains your already created Classifier Pipeline\n",
    "    Inputs:\n",
    "      - X (mandatory) - tokenized data for training - Pandas Series\n",
    "      - y (mandatory) - Multilabels 0|1 - Pandas Dataset\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - trained model'''\n",
    "    if verbose:\n",
    "        print('###train function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Train test split\n",
    "    #Split makes randomization, so random_state parameter was set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=42)\n",
    "    if (X_train.shape[0] + X_test.shape[0]) == X.shape[0]:\n",
    "        if verbose:\n",
    "            print('data split into train and text seems OK')\n",
    "    else:\n",
    "        raise Exception('something went wrong when splitting the data')\n",
    "        \n",
    "    #2.fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # output model test results\n",
    "    y_pred = pipeline_lnsv.predict(X_test)\n",
    "    if verbose:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=True)\n",
    "    else:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=False)\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric < 0.6:\n",
    "            raise Exception('something is wrong, model is predicting poorly')\n",
    "\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*classifier was trained!')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def export_model(model,\n",
    "                 file_name='classifier.pkl'\n",
    "                 verbose=False):\n",
    "    '''This function writes your already trained Classifiear as a Picke Binary\n",
    "    file.\n",
    "    Inputs:\n",
    "      - model (mandatory) - your already trained Classifiear - Python Object\n",
    "      - file_name (optional) - the name of the file to be created (default:\n",
    "         'classifier.pkl')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "       Output: return True if everything runs OK\n",
    "      ''' \n",
    "    if verbose:\n",
    "        print('###export_model function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Export model as a pickle file\n",
    "    file_name = file_name\n",
    "\n",
    "    #writing the file\n",
    "    with open (file_name, 'wb') as pk_writer: \n",
    "        pickle.dump(model, pk_writer)\n",
    "\n",
    "    #reading the file\n",
    "    #with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    #    model = pickle.load(pk_reader)\n",
    "    \n",
    "    if verbose:\n",
    "        print('*trained Classifier was exported')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def run_pipeline(data_file='sqlite:///Messages.db', \n",
    "                 verbose=False):\n",
    "    '''This function is a caller: it calls load, build, train and save modules\n",
    "    Inputs:\n",
    "      - data_file (optional) - complete path to the SQLite datafile to be \n",
    "        processed - (default='sqlite:///Messages.db')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output: return True if everything runs OK\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###run_pipeline function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Run ETL pipeline\n",
    "    X, y = load_data(data_file, \n",
    "                     verbose=verbose)\n",
    "    #2.Build model pipeline\n",
    "    model = build_model(verbose=verbose)\n",
    "    #3.Train model pipeline\n",
    "    model = train(X, \n",
    "                  y, \n",
    "                  model, \n",
    "                  verbose=verbose)\n",
    "    # save model\n",
    "    export_model(model,\n",
    "                 verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(data_file='sqlite:///Messages.db',\n",
    "             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_file = sys.argv[1]  # get filename of dataset\n",
    "    run_pipeline(data_file)  # run data pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
