{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Condensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#measuring time and making basic math\n",
    "from time import time\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "#import statistics\n",
    "\n",
    "#my own ETL pipeline\n",
    "#import process_data as pr\n",
    "\n",
    "#dealing with datasets and showing content\n",
    "import pandas as pd\n",
    "#import pprint as pp\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#natural language toolkit\n",
    "#from nltk.tokenize import word_tokenize \n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#REGEX toolkit\n",
    "#import re\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.ensemble import RandomForestClassifier #need MOClassifier!\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Machine Learning Metrics\n",
    "#from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#pickling tool\n",
    "import pickle\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing tables in my SQLite database: ['Messages']\n"
     ]
    }
   ],
   "source": [
    "#importing MySQL to Pandas - load data from database\n",
    "engine = create_engine('sqlite:///Messages.db', poolclass=pool.NullPool) #, echo=True)\n",
    "\n",
    "#retrieving tables names from my DB\n",
    "#https://stackoverflow.com/questions/6473925/sqlalchemy-getting-a-list-of-tables\n",
    "inspector = inspect(engine)\n",
    "if verbose:\n",
    "    print('existing tables in my SQLite database:', inspector.get_table_names())\n",
    "\n",
    "connection = engine.connect()\n",
    "df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "connection.close()\n",
    "df.name = 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all labels are blank in 6317 rows\n"
     ]
    }
   ],
   "source": [
    "#I.Prepare the data\n",
    "if verbose:\n",
    "    print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining rows: 19928\n"
     ]
    }
   ],
   "source": [
    "df = df[df['if_blank'] == 0]\n",
    "if verbose:\n",
    "    print('remaining rows:', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removal complete!\n"
     ]
    }
   ],
   "source": [
    "#Verifying if removal was complete\n",
    "if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "    if verbose:\n",
    "        print('removal complete!')\n",
    "else:\n",
    "    raise Exception('something went wrong with rows removal before training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "found 6 rows with no tokens\n",
      "*after removal, found 0 rows with no tokens\n",
      "now I have 19922 rows to train\n",
      "process time:22 seconds\n"
     ]
    }
   ],
   "source": [
    "#Premature Tokenization Strategy (pre-tokenizer)\n",
    "#Pre-Tokenizer + not removing provisory tokenized column\n",
    "start = time()\n",
    "\n",
    "#inserting a tokenized column\n",
    "try:\n",
    "    df = df.drop('tokenized', axis=1)\n",
    "except KeyError:\n",
    "    print('OK')\n",
    "df.insert(1, 'tokenized', np.nan)\n",
    "\n",
    "#tokenizing over the provisory\n",
    "df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "\n",
    "#removing NaN over provisory (if istill exist)\n",
    "df = df[df['tokenized'].notnull()]\n",
    "\n",
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "if verbose:\n",
    "    print('found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "if verbose:\n",
    "    print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "#I will drop 'message' column\n",
    "try:\n",
    "    df = df.drop('message', axis=1)\n",
    "except KeyError:\n",
    "    if verbose:\n",
    "        print('OK')\n",
    "\n",
    "if verbose:\n",
    "    print('now I have {} rows to train'.format(df.shape[0]))\n",
    "\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('process time:{:.0f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function group_check started\n",
      "  - count for main class:aid_related, 10877 entries\n",
      "  - for main, without any sub-categories,  3515 entries\n",
      "  - for subcategories,  7388 entries\n",
      "  - for lost parent sub-categories,  26 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.1218s\n",
      "###function group_check started\n",
      "  - count for main class:weather_related, 7304 entries\n",
      "  - for main, without any sub-categories,  1359 entries\n",
      "  - for subcategories,  5945 entries\n",
      "  - for lost parent sub-categories,  0 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0604s\n",
      "###function group_check started\n",
      "  - count for main class:infrastructure_related, 1705 entries\n",
      "  - for main, without any sub-categories,  679 entries\n",
      "  - for subcategories,  2926 entries\n",
      "  - for lost parent sub-categories,  1900 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0519s\n",
      "###function group_check started\n",
      "  - count for main class:related, 19922 entries\n",
      "  - for main, without any sub-categories,  9436 entries\n",
      "  - for subcategories,  10486 entries\n",
      "  - for lost parent sub-categories,  0 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0379s\n",
      "Data Consistency performed\n",
      "process time:0 seconds\n"
     ]
    }
   ],
   "source": [
    "#Database Data Consistency Fix\n",
    "start = time()\n",
    "\n",
    "#correction for aid_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='aid',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for weather_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='wtr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for infrastrucutre_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='ifr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for related(considering that the earlier were already corrected)\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='main',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('Data Consistency performed')\n",
    "    print('process time:{:.0f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#II.Break the data\n",
    "#X is the Training Text Column\n",
    "X = df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*child_alone -> only zeroes training column!\n",
      "['child_alone']\n",
      "Dataset breaked into X-Training Text Column and Y-Multilabels for Classification\n"
     ]
    }
   ],
   "source": [
    "#y is the Classification labels\n",
    "#I REMOVED \"related\" column from my labels, as it is impossible to train it!\n",
    "y = df[df.columns[4:]]\n",
    "#y = df[df.columns[5:]]\n",
    "\n",
    "#remove_lst = []\n",
    "\n",
    "#for column in y.columns:\n",
    "#    col = y[column]\n",
    "#    if (col == 0).all():\n",
    "#        if verbose:\n",
    "#            print('*{} -> only zeroes training column!'.format(column))\n",
    "#        remove_lst.append(column)\n",
    "#    else:\n",
    "        #print('*{} -> column OK'.format(column))\n",
    "#        pass\n",
    "\n",
    "#if verbose:\n",
    "#    print(remove_lst)\n",
    "\n",
    "#y = y.drop(remove_lst, axis=1)\n",
    "\n",
    "if y.shape[1] == 36:\n",
    "    if verbose:\n",
    "        print('y dataset has 36 labels')\n",
    "else:\n",
    "    raise Exception('something went wrong, dataset has {} labels instead of 36'.format(y.shape[1]))\n",
    "\n",
    "if verbose:\n",
    "    print('Dataset breaked into X-Training Text Column and Y-Multilabels for Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#III.Slit the data\n",
    "#Split makes randomization, so random_state parameter was set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split into train and text seems OK\n"
     ]
    }
   ],
   "source": [
    "if (X_train.shape[0] + X_test.shape[0]) == X.shape[0]:\n",
    "    if verbose:\n",
    "        print('data split into train and text seems OK')\n",
    "else:\n",
    "    raise Exception('something went wrong when splitting the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training started\n",
      "LINEAR SUPPORT VECTOR MACHINE - process time:16.34 seconds\n"
     ]
    }
   ],
   "source": [
    "#IV. Train your Classifier\n",
    "#Classifier is Support Vector Machine\n",
    "if verbose:\n",
    "    print('Classifier training started')\n",
    "\n",
    "start = time()\n",
    "    \n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "feats = TfidfVectorizer(analyzer='word', \n",
    "                        tokenizer=dummy, \n",
    "                        preprocessor=dummy,\n",
    "                        token_pattern=None,\n",
    "                        ngram_range=(1, 3))\n",
    "\n",
    "classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                        random_state=42))\n",
    "\n",
    "pipeline_lnsv = Pipeline([('vect', feats),\n",
    "                          ('clf', classif)])\n",
    "\n",
    "pipeline_lnsv.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "if verbose:\n",
    "    print('LINEAR SUPPORT VECTOR MACHINE - process time:{:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function scores_report started\n",
      "using top 10 labels\n",
      "######################################################\n",
      "*aid_related -> label iloc[2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.56      0.64      2313\n",
      "           1       0.69      0.84      0.76      2668\n",
      "\n",
      "    accuracy                           0.71      4981\n",
      "   macro avg       0.72      0.70      0.70      4981\n",
      "weighted avg       0.72      0.71      0.70      4981\n",
      "\n",
      "######################################################\n",
      "*weather_related -> label iloc[26]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      3109\n",
      "           1       0.80      0.79      0.80      1872\n",
      "\n",
      "    accuracy                           0.85      4981\n",
      "   macro avg       0.84      0.84      0.84      4981\n",
      "weighted avg       0.85      0.85      0.85      4981\n",
      "\n",
      "######################################################\n",
      "*direct_report -> label iloc[33]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      3726\n",
      "           1       0.63      0.62      0.62      1255\n",
      "\n",
      "    accuracy                           0.81      4981\n",
      "   macro avg       0.75      0.75      0.75      4981\n",
      "weighted avg       0.81      0.81      0.81      4981\n",
      "\n",
      "######################################################\n",
      "*request -> label iloc[0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      3884\n",
      "           1       0.71      0.71      0.71      1097\n",
      "\n",
      "    accuracy                           0.87      4981\n",
      "   macro avg       0.81      0.81      0.81      4981\n",
      "weighted avg       0.87      0.87      0.87      4981\n",
      "\n",
      "######################################################\n",
      "*other_aid -> label iloc[16]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      4163\n",
      "           1       0.45      0.36      0.40       818\n",
      "\n",
      "    accuracy                           0.82      4981\n",
      "   macro avg       0.66      0.64      0.65      4981\n",
      "weighted avg       0.81      0.82      0.81      4981\n",
      "\n",
      "######################################################\n",
      "*food -> label iloc[9]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      4266\n",
      "           1       0.78      0.81      0.79       715\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.88      0.88      0.88      4981\n",
      "weighted avg       0.94      0.94      0.94      4981\n",
      "\n",
      "######################################################\n",
      "*earthquake -> label iloc[30]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      4384\n",
      "           1       0.86      0.78      0.82       597\n",
      "\n",
      "    accuracy                           0.96      4981\n",
      "   macro avg       0.92      0.88      0.90      4981\n",
      "weighted avg       0.96      0.96      0.96      4981\n",
      "\n",
      "######################################################\n",
      "*storm -> label iloc[28]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      4371\n",
      "           1       0.73      0.72      0.72       610\n",
      "\n",
      "    accuracy                           0.93      4981\n",
      "   macro avg       0.84      0.84      0.84      4981\n",
      "weighted avg       0.93      0.93      0.93      4981\n",
      "\n",
      "######################################################\n",
      "*shelter -> label iloc[10]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      4410\n",
      "           1       0.73      0.69      0.71       571\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.85      0.83      0.84      4981\n",
      "weighted avg       0.93      0.94      0.93      4981\n",
      "\n",
      "######################################################\n",
      "*floods -> label iloc[27]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      4412\n",
      "           1       0.81      0.65      0.72       569\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.88      0.81      0.84      4981\n",
      "weighted avg       0.94      0.94      0.94      4981\n",
      "\n",
      "###Model metrics for 10 labels:\n",
      " Accuracy: 0.705 (70.5%)\n",
      " Precision: 0.719 (71.9%)\n",
      " Recall: 0.697 (69.7%)\n",
      "\n",
      "###Worst metrics:\n",
      " Accuracy: 0.398 (39.8%) for other_aid\n",
      " Precision: 0.447 (44.7%) for other_aid\n",
      " Recall: 0.358 (35.8%) for other_aid\n",
      "process time:0.1956 seconds\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline_lnsv.predict(X_test)\n",
    "if verbose:\n",
    "    metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                 y_pred,\n",
    "                                 best_10=True,\n",
    "                                 verbose=True)\n",
    "else:\n",
    "    metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                           y_pred,\n",
    "                                           best_10=True,\n",
    "                                           verbose=False)\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric < 0.6:\n",
    "        raise Exception('something is wrong, model is predicting poorly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_adab.fit(X_train, y_train)\n",
    "\n",
    "file_name = 'classifier.pkl'\n",
    "\n",
    "#writing the file\n",
    "with open (file_name, 'wb') as pk_writer: \n",
    "    pickle.dump(pipeline_lnsv, pk_writer)\n",
    "\n",
    "#reading the file\n",
    "with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    pipeline_lnsv = pickle.load(pk_reader)\n",
    "    \n",
    "#pipeline_lnsv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the notebook to complete `train.py`\n",
    "\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pickling tool\n",
    "import pickle\n",
    "\n",
    "#only a dummy function, as I pre-tokenize my data\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def load_data(data_file, \n",
    "              verbose=False):\n",
    "    '''This function takes a path for a SQLite table and returns processed data\n",
    "    for training a Machine Learning Classifier\n",
    "    Inputs:\n",
    "      - data_file (mandatory) - full path for SQLite table - text string\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Oputputs:\n",
    "      - X - tokenized text X-training - Pandas Series\n",
    "      - y - y-multilabels 0|1 - Pandas Dataframe'''\n",
    "    if verbose:\n",
    "        print('###load_data function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.read in file\n",
    "    #importing MySQL to Pandas - load data from database\n",
    "    engine = create_engine(data_file, poolclass=pool.NullPool) #, echo=True)\n",
    "    #retrieving tables names from my DB\n",
    "    inspector = inspect(engine)\n",
    "    if verbose:\n",
    "        print('existing tables in my SQLite database:', inspector.get_table_names())\n",
    "    connection = engine.connect()\n",
    "    df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "    connection.close()\n",
    "    df.name = 'df'\n",
    "    \n",
    "    #2.clean data\n",
    "    #2.1.Elliminate rows with all-blank labels\n",
    "    if verbose:\n",
    "        print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))\n",
    "    df = df[df['if_blank'] == 0]\n",
    "    if verbose:\n",
    "        print('remaining rows:', df.shape[0])\n",
    "    #Verifying if removal was complete\n",
    "    if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "        if verbose:\n",
    "            print('removal complete!')\n",
    "        else:\n",
    "            raise Exception('something went wrong with rows removal before training')\n",
    "            \n",
    "    #2.2.Premature Tokenization Strategy (pre-tokenizer)\n",
    "    #Pre-Tokenizer + not removing provisory tokenized column\n",
    "    #inserting a tokenized column\n",
    "    try:\n",
    "        df = df.drop('tokenized', axis=1)\n",
    "    except KeyError:\n",
    "        print('OK')\n",
    "    df.insert(1, 'tokenized', np.nan)\n",
    "    #tokenizing over the provisory\n",
    "    df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "    #removing NaN over provisory (if istill exist)\n",
    "    df = df[df['tokenized'].notnull()]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('found {} rows with no tokens'.format(empty_tokens))\n",
    "    df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "    #I will drop the original 'message' column\n",
    "    try:\n",
    "        df = df.drop('message', axis=1)\n",
    "    except KeyError:\n",
    "        if verbose:\n",
    "            print('OK')\n",
    "    if verbose:\n",
    "        print('now I have {} rows to train'.format(df.shape[0]))\n",
    "\n",
    "    #2.3.Database Data Consistency Check/Fix\n",
    "    #correction for aid_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='aid',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for weather_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='wtr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for infrastrucutre_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='ifr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for related(considering that the earlier were already corrected)\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='main',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    \n",
    "    #load to database <-I don't know for what it is\n",
    "    \n",
    "    #3.Define features and label arrays (break the data)\n",
    "    #3.1.X is the Training Text Column\n",
    "    X = df['tokenized']\n",
    "    #3.2.y is the Classification labels\n",
    "    #I REMOVED \"related\" column from my labels, as it is impossible to train it!\n",
    "    y = df[df.columns[5:]]\n",
    "    remove_lst = []\n",
    "\n",
    "    for column in y.columns:\n",
    "        col = y[column]\n",
    "        if (col == 0).all():\n",
    "            if verbose:\n",
    "                print('*{} -> only zeroes training column!'.format(column))\n",
    "            remove_lst.append(column)\n",
    "        else:\n",
    "            #print('*{} -> column OK'.format(column))\n",
    "            pass\n",
    "        \n",
    "    if verbose:\n",
    "        print(remove_lst)\n",
    "    y = y.drop(remove_lst, axis=1)\n",
    "    \n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*dataset breaked into X-Training Text Column and Y-Multilabels')    \n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def build_model(verbose=False):\n",
    "    '''This function builds the Classifier Pipeline, for future fitting\n",
    "    Inputs:\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - model_pipeline for your Classifiear (untrained)\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###build_model function started')\n",
    "    start = time()\n",
    "    \n",
    "    #1.text processing and model pipeline\n",
    "    #(text processing was made at a earlier step, at Load Data function)\n",
    "    feats = TfidfVectorizer(analyzer='word', \n",
    "                            tokenizer=dummy, \n",
    "                            preprocessor=dummy,\n",
    "                            token_pattern=None,\n",
    "                            ngram_range=(1, 3))\n",
    "    \n",
    "    classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                            random_state=42))\n",
    "    \n",
    "    model_pipeline = Pipeline([('vect', feats),\n",
    "                               ('clf', classif)])\n",
    "    \n",
    "    #define parameters for GridSearchCV (parameters already defined)\n",
    "    #create gridsearch object and return as final model pipeline (made at pipeline preparation)\n",
    "    #obs: for better performance, I pre-tokenized my data. And GridSearch was runned on Jupyter,\n",
    "    #     and the best parameters where adjusted, just to save processing time during code execution.\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*Linear Support Vector Machine pipeline was created')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def train(X, \n",
    "          y, \n",
    "          model, \n",
    "          verbose=False):\n",
    "    '''This function trains your already created Classifier Pipeline\n",
    "    Inputs:\n",
    "      - X (mandatory) - tokenized data for training - Pandas Series\n",
    "      - y (mandatory) - Multilabels 0|1 - Pandas Dataset\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - trained model'''\n",
    "    if verbose:\n",
    "        print('###train function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Train test split\n",
    "    #Split makes randomization, so random_state parameter was set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=42)\n",
    "    if (X_train.shape[0] + X_test.shape[0]) == X.shape[0]:\n",
    "        if verbose:\n",
    "            print('data split into train and text seems OK')\n",
    "    else:\n",
    "        raise Exception('something went wrong when splitting the data')\n",
    "        \n",
    "    #2.fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # output model test results\n",
    "    y_pred = pipeline_lnsv.predict(X_test)\n",
    "    if verbose:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=True)\n",
    "    else:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=False)\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric < 0.6:\n",
    "            raise Exception('something is wrong, model is predicting poorly')\n",
    "\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*classifier was trained!')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def export_model(model,\n",
    "                 file_name='classifier.pkl'\n",
    "                 verbose=False):\n",
    "    '''This function writes your already trained Classifiear as a Picke Binary\n",
    "    file.\n",
    "    Inputs:\n",
    "      - model (mandatory) - your already trained Classifiear - Python Object\n",
    "      - file_name (optional) - the name of the file to be created (default:\n",
    "         'classifier.pkl')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "       Output: return True if everything runs OK\n",
    "      ''' \n",
    "    if verbose:\n",
    "        print('###export_model function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Export model as a pickle file\n",
    "    file_name = file_name\n",
    "\n",
    "    #writing the file\n",
    "    with open (file_name, 'wb') as pk_writer: \n",
    "        pickle.dump(model, pk_writer)\n",
    "\n",
    "    #reading the file\n",
    "    #with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    #    model = pickle.load(pk_reader)\n",
    "    \n",
    "    if verbose:\n",
    "        print('*trained Classifier was exported')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def run_pipeline(data_file='sqlite:///Messages.db', \n",
    "                 verbose=False):\n",
    "    '''This function is a caller: it calls load, build, train and save modules\n",
    "    Inputs:\n",
    "      - data_file (optional) - complete path to the SQLite datafile to be \n",
    "        processed - (default='sqlite:///Messages.db')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output: return True if everything runs OK\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###run_pipeline function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Run ETL pipeline\n",
    "    X, y = load_data(data_file, \n",
    "                     verbose=verbose)\n",
    "    #2.Build model pipeline\n",
    "    model = build_model(verbose=verbose)\n",
    "    #3.Train model pipeline\n",
    "    model = train(X, \n",
    "                  y, \n",
    "                  model, \n",
    "                  verbose=verbose)\n",
    "    # save model\n",
    "    export_model(model,\n",
    "                 verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###run_pipeline function started\n",
      "###load_data function started\n",
      "existing tables in my SQLite database: ['Messages']\n",
      "all labels are blank in 6317 rows\n",
      "remaining rows: 19928\n",
      "removal complete!\n",
      "OK\n",
      "found 6 rows with no tokens\n",
      "*after removal, found 0 rows with no tokens\n",
      "now I have 19922 rows to train\n",
      "###function group_check started\n",
      "  - count for main class:aid_related, 10877 entries\n",
      "  - for main, without any sub-categories,  3515 entries\n",
      "  - for subcategories,  7388 entries\n",
      "  - for lost parent sub-categories,  26 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0843s\n",
      "###function group_check started\n",
      "  - count for main class:weather_related, 7304 entries\n",
      "  - for main, without any sub-categories,  1359 entries\n",
      "  - for subcategories,  5945 entries\n",
      "  - for lost parent sub-categories,  0 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0388s\n",
      "###function group_check started\n",
      "  - count for main class:infrastructure_related, 1705 entries\n",
      "  - for main, without any sub-categories,  679 entries\n",
      "  - for subcategories,  2926 entries\n",
      "  - for lost parent sub-categories,  1900 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0379s\n",
      "###function group_check started\n",
      "  - count for main class:related, 19922 entries\n",
      "  - for main, without any sub-categories,  9436 entries\n",
      "  - for subcategories,  10486 entries\n",
      "  - for lost parent sub-categories,  0 entries\n",
      "    *correcting, new count: 0 entries\n",
      "elapsed time: 0.0399s\n",
      "*child_alone -> only zeroes training column!\n",
      "['child_alone']\n",
      "Dataset breaked into X-Training Text Column and Y-Multilabels for Classification\n",
      "process time:26 seconds\n",
      "###build_model function started\n",
      "LINEAR SUPPORT VECTOR MACHINE pipeline created - process time:0 seconds\n",
      "###train function started\n",
      "data split into train and text seems OK\n",
      "###function scores_report started\n",
      "using top 10 labels\n",
      "######################################################\n",
      "*aid_related -> label iloc[2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.56      0.64      2313\n",
      "           1       0.69      0.84      0.76      2668\n",
      "\n",
      "    accuracy                           0.71      4981\n",
      "   macro avg       0.72      0.70      0.70      4981\n",
      "weighted avg       0.72      0.71      0.70      4981\n",
      "\n",
      "######################################################\n",
      "*weather_related -> label iloc[26]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      3109\n",
      "           1       0.80      0.79      0.80      1872\n",
      "\n",
      "    accuracy                           0.85      4981\n",
      "   macro avg       0.84      0.84      0.84      4981\n",
      "weighted avg       0.85      0.85      0.85      4981\n",
      "\n",
      "######################################################\n",
      "*direct_report -> label iloc[33]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      3726\n",
      "           1       0.63      0.62      0.62      1255\n",
      "\n",
      "    accuracy                           0.81      4981\n",
      "   macro avg       0.75      0.75      0.75      4981\n",
      "weighted avg       0.81      0.81      0.81      4981\n",
      "\n",
      "######################################################\n",
      "*request -> label iloc[0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      3884\n",
      "           1       0.71      0.71      0.71      1097\n",
      "\n",
      "    accuracy                           0.87      4981\n",
      "   macro avg       0.81      0.81      0.81      4981\n",
      "weighted avg       0.87      0.87      0.87      4981\n",
      "\n",
      "######################################################\n",
      "*other_aid -> label iloc[16]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      4163\n",
      "           1       0.45      0.36      0.40       818\n",
      "\n",
      "    accuracy                           0.82      4981\n",
      "   macro avg       0.66      0.64      0.65      4981\n",
      "weighted avg       0.81      0.82      0.81      4981\n",
      "\n",
      "######################################################\n",
      "*food -> label iloc[9]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      4266\n",
      "           1       0.78      0.81      0.79       715\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.88      0.88      0.88      4981\n",
      "weighted avg       0.94      0.94      0.94      4981\n",
      "\n",
      "######################################################\n",
      "*earthquake -> label iloc[30]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      4384\n",
      "           1       0.86      0.78      0.82       597\n",
      "\n",
      "    accuracy                           0.96      4981\n",
      "   macro avg       0.92      0.88      0.90      4981\n",
      "weighted avg       0.96      0.96      0.96      4981\n",
      "\n",
      "######################################################\n",
      "*storm -> label iloc[28]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      4371\n",
      "           1       0.73      0.72      0.72       610\n",
      "\n",
      "    accuracy                           0.93      4981\n",
      "   macro avg       0.84      0.84      0.84      4981\n",
      "weighted avg       0.93      0.93      0.93      4981\n",
      "\n",
      "######################################################\n",
      "*shelter -> label iloc[10]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      4410\n",
      "           1       0.73      0.69      0.71       571\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.85      0.83      0.84      4981\n",
      "weighted avg       0.93      0.94      0.93      4981\n",
      "\n",
      "######################################################\n",
      "*floods -> label iloc[27]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      4412\n",
      "           1       0.81      0.65      0.72       569\n",
      "\n",
      "    accuracy                           0.94      4981\n",
      "   macro avg       0.88      0.81      0.84      4981\n",
      "weighted avg       0.94      0.94      0.94      4981\n",
      "\n",
      "###Model metrics for 10 labels:\n",
      " Accuracy: 0.705 (70.5%)\n",
      " Precision: 0.719 (71.9%)\n",
      " Recall: 0.697 (69.7%)\n",
      "\n",
      "###Worst metrics:\n",
      " Accuracy: 0.398 (39.8%) for other_aid\n",
      " Precision: 0.447 (44.7%) for other_aid\n",
      " Recall: 0.358 (35.8%) for other_aid\n",
      "process time:0.2176 seconds\n",
      "process time:20 seconds\n",
      "###export_model function started\n",
      "process time:16 seconds\n",
      "process time:16 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline(data_file='sqlite:///Messages.db',\n",
    "             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "Could not parse rfc1738 URL from string '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-87191e06926a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# get filename of dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrun_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run data pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-704142d28920>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[1;34m(data_file, verbose)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#1.Run ETL pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     X, y = load_data(data_file, \n\u001b[1;32m---> 15\u001b[1;33m                      verbose=verbose)\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m#2.Build model pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5a624f6f76a5>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(data_file, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m#1.read in file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m#importing MySQL to Pandas - load data from database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoolclass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNullPool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, echo=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;31m#retrieving tables names from my DB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0minspector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py\u001b[0m in \u001b[0;36mwarned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                     )\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\create.py\u001b[0m in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;31m# create url.URL object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instantiate_plugins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\url.py\u001b[0m in \u001b[0;36mmake_url\u001b[1;34m(name_or_url)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_parse_rfc1738_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mname_or_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\url.py\u001b[0m in \u001b[0;36m_parse_rfc1738_args\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         raise exc.ArgumentError(\n\u001b[1;32m--> 756\u001b[1;33m             \u001b[1;34m\"Could not parse rfc1738 URL from string '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m         )\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: Could not parse rfc1738 URL from string '-f'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_file = sys.argv[1]  # get filename of dataset\n",
    "    run_pipeline(data_file)  # run data pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
