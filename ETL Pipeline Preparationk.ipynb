{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import NullPool\n",
    "#import re\n",
    "import udacourse2 #my library!\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_begin = time()\n",
    "\n",
    "# load messages dataset\n",
    "messages = pd.read_csv('messages.csv', index_col='id')\n",
    "#messages['id'] = messages.index\n",
    "#messages.info()\n",
    "#messages.index\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load categories dataset\n",
    "categories = pd.read_csv('categories.csv', index_col='id')\n",
    "#categories.info()\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove duplicates.\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop the duplicates.\n",
    "- Confirm duplicates were removed\n",
    "- Index duplicated [here](https://stackoverflow.com/questions/35084071/concat-dataframe-reindexing-only-valid-with-uniquely-valued-index-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages.index.duplicated(keep='first')\n",
    "#drop index duplicated messages\n",
    "messages = messages.loc[~messages.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of remaining duplicated messages\n",
    "print(messages[messages.duplicated()].shape[0])\n",
    "#drop duplicates\n",
    "messages = messages.drop_duplicates()\n",
    "print(messages.shape[0])\n",
    "# check number of duplicates\n",
    "messages[messages.duplicated()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps\n",
    "\n",
    "- used SQL-type of relations, having the messages dataframe as refference\n",
    "- documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "df = pd.merge(messages, categories, left_index=True, right_index=True, how='left')\n",
    "print(df.shape[0])\n",
    "df.name = 'df'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split `categories` into separate category columns.\n",
    "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
    "- Use the first row of categories dataframe to create column names for the categories data.\n",
    "- Rename columns of `categories` with new column names.\n",
    "\n",
    "Solution [here](https://stackoverflow.com/questions/42049147/convert-list-to-pandas-dataframe-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the 36 individual category columns\n",
    "jumber = df['categories'].iloc[0]\n",
    "categories = jumber.split(sep=';')\n",
    "#{'categories': categories}\n",
    "categories = pd.DataFrame({'categories': categories})\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first row of the categories dataframe\n",
    "row = categories.iloc[0]\n",
    "row['categories'][:-2]\n",
    "\n",
    "[cat[:-2] for cat in categories['categories']]\n",
    "\n",
    "# use this row to extract a list of new column names for categories.\n",
    "# one way is to apply a lambda function that takes everything \n",
    "# up to the second to last character of each string with slicing\n",
    "category_colnames = [cat[:-2] for cat in categories['categories']]\n",
    "print(category_colnames)\n",
    "\n",
    "# rename the columns of `categories`\n",
    "#categories.columns = category_colnames\n",
    "#categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alert flag, if there was no category inserted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['if_blank'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new columns with zero value\n",
    "for colname in category_colnames:\n",
    "    df[colname] = 0\n",
    "\n",
    "#df.columns\n",
    "print('new shape:',df.shape[1])\n",
    "#df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = df['categories'].iloc[0]\n",
    "alfa = set(cell.split(sep=';'))\n",
    "alfa\n",
    "\n",
    "for beta in alfa:\n",
    "    if beta.find('1') != -1:\n",
    "        print(beta[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert category values to just numbers 0 or 1.\n",
    "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
    "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = time()\n",
    "\n",
    "filtered_cols = ['categories', 'if_blank', 'related', 'request', 'offer', 'aid_related', 'medical_help', \n",
    "       'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', \n",
    "       'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', \n",
    "       'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', \n",
    "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
    "\n",
    "df[filtered_cols] = df[filtered_cols].apply(lambda x: udacourse2.fn_test(x, verbose=False), axis=1)\n",
    "\n",
    "spent = time() - begin\n",
    "print('elapsed time: {:.1f}s ({}min, {:.4f}sec)'.format(spent, math.trunc((spent)/60), math.fmod(spent, 60)))\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace `categories` column in `df` with new category columns.\n",
    "- Drop the categories column from the df dataframe since it is no longer needed.\n",
    "- Concatenate df and categories data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('categories', axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save the clean dataset into an sqlite database.\n",
    "You can do this with pandas [`to_sql` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) combined with the SQLAlchemy library. Remember to import SQLAlchemy's `create_engine` in the first cell of this notebook to use it below.\n",
    "\n",
    "drop table function [here](import logging\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better way to do this, [here](https://stackoverflow.com/questions/8645250/how-to-close-sqlalchemy-connection-in-mysql)\n",
    "\n",
    "- according to the answerer, \"Engine is a **factory** for connections as well as a pool of connections, not the connection itself\";\n",
    "- so, to avoid the problem of couldn´t closse the connection, as other members of the poll remaining asking for transactions, the answerer recommends to use **poolclass=NulPool**;\n",
    "- as we are not dealing with something that really needs a pool (only one transaction per time for us is enough), let´s do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = create_engine('sqlite:///Messages.db', poolclass=NullPool) #, echo=True)\n",
    "connection = database.connect()\n",
    "\n",
    "#attempt to save my dataframe to SQLite\n",
    "try:\n",
    "    df.to_sql('Messages', database, index=False, if_exists='replace')\n",
    "except ValueError:\n",
    "    print('something went wrong when was writing data do SQLite')\n",
    "    \n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spent = time() - gen_begin\n",
    "print('total elapsed time: {:.1f}s ({}min, {:.4f}sec)'.format(spent, math.trunc((spent)/60), math.fmod(spent, 60)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use this notebook to complete `etl_pipeline.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database based on new datasets specified by the user. Alternatively, you can complete `etl_pipeline.py` in the classroom on the `Project Workspace IDE` coming later.\n",
    "\n",
    "*this was made into a new notebook named `ETL Pipeline Test.py`, and at `etl_pipeline.py`*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Later tests over the generated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the valid registers for one column, according to a **criteria**:\n",
    "\n",
    "- `fn_count_valids` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test for the function for counting the valid labels for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df.shape[0]\n",
    "field = 'if_blank'\n",
    "count = udacourse2.fn_count_valids(dataset=df, field=field, criteria=True)\n",
    "percent = 100. * (count / total)\n",
    "print('{}:{} ({:.1f}%)'.format(field, count, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning this into a function\n",
    "\n",
    "- `fn_valids_report` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udacourse2.fn_valids_report(dataset=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Labels columns, by **hierarchical structure**\n",
    "\n",
    "*See discussion above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_lst = ['related', 'request', 'offer', 'aid_related', 'infrastructure_related', 'weather_related', \n",
    "              'direct_report']\n",
    "\n",
    "aid_lst = ['food', 'shelter', 'water', 'death', 'refugees', 'money', 'security', 'military', 'clothing', \n",
    "           'tools', 'missing_people', 'child_alone', 'search_and_rescue', 'medical_help', 'medical_products', \n",
    "           'aid_centers', 'other_aid']\n",
    "\n",
    "weather_lst = ['earthquake', 'storm', 'floods', 'fire', 'cold', 'other_weather']\n",
    "\n",
    "infrastructure_lst = ['buildings', 'transport', 'hospitals', 'electricity', 'shops', 'other_infrastructure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To concatenate lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['a', 'b']\n",
    "b = ['c', 'd']\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the concept into our project:\n",
    "    \n",
    "- `fn_count_valids` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_list = expand_lst + aid_lst + weather_lst + infrastructure_lst\n",
    "\n",
    "verbose = False\n",
    "total = df.shape[0]\n",
    "counts = []\n",
    "\n",
    "for field in expand_list:\n",
    "    count = udacourse2.fn_count_valids(dataset=df, field=field)\n",
    "    percent = 100. * (count / total)\n",
    "    counts.append((count, field, percent))\n",
    "    if verbose:\n",
    "        print('{}:{} ({:.1f}%)'.format(field, count, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order Tupples [here](https://www.pythoncentral.io/how-to-sort-a-list-tuple-or-object-with-sorted-in-python/):\n",
    "\n",
    "- i will need it to create an **ordered report** about labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tuples = sorted(counts, key=udacourse2.fn_getKey, reverse=True)\n",
    "\n",
    "i=1\n",
    "c=2\n",
    "max_c=3\n",
    "\n",
    "for cat in sorted_tuples:\n",
    "    count, field, percent = cat\n",
    "    print('{}-{}:{} ({:.1f}%)'.format(i, field, count, percent))\n",
    "    if c > max_c:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning this into a function:\n",
    "\n",
    "- `fn_labels_report`created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = udacourse2.fn_labels_report(dataset=df,\n",
    "                                     data_ret=True,\n",
    "                                     max_c=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main labels counting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- pie at Plotly documentation charts [here](https://plotly.com/python/pie-charts/)\n",
    ">- using Plotly Express (an easier way to plot graphics)\n",
    ">- **Pie** charts are nice to show **relative** percentages (how in general, the labels are homogeneously distributed under a Dadatframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuples_main['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuples_main['percentage'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = px.data.gapminder().query(\"year == 2007\").query(\"continent == 'Europe'\")\n",
    "#df.loc[df['pop'] < 2.e6, 'country'] = 'Other countries' # Represent only large countries\n",
    "#fig = px.pie(df, values='pop', names='country', title='Population of European continent')\n",
    "#fig.show()\n",
    "\n",
    "tuples_main = udacourse2.fn_labels_report(dataset=df,\n",
    "                                          label_filter='main',\n",
    "                                          data_ret=True,\n",
    "                                          max_c=False)\n",
    "\n",
    "fig = px.pie(tuples_main,\n",
    "             names='label',\n",
    "             values='percentage',\n",
    "             title='Main Categories - relative percentages')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main labels Total:\n",
    "    \n",
    ">- new use of Plotly Express, this time for **Bar Charts** [here](https://plotly.com/python/bar-charts/)\n",
    ">- the **blue** color represent `this most dominant category, fragmented under a lot of **subcategories**, shown below\n",
    ">- use of colors for a better representation of graphs [here](https://plotly.com/python/discrete-color/)\n",
    ">- found a way to export to json [here](https://stackoverflow.com/questions/57769581/save-plot-ly-json-to-a-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_main = udacourse2.fn_labels_report(dataset=df,\n",
    "                                          label_filter='main',\n",
    "                                          data_ret=True,\n",
    "                                          max_c=False)\n",
    "fig = px.bar(tuples_main, \n",
    "             x='label', \n",
    "             y='percentage',\n",
    "             title='Main Categories - total percentages',\n",
    "             color=['#00D', 'goldenrod', 'green', 'red'], \n",
    "             color_discrete_map=\"identity\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On `related`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_main = udacourse2.fn_labels_report(dataset=df,\n",
    "                                          label_filter='related',\n",
    "                                          data_ret=True,\n",
    "                                          max_c=False)\n",
    "fig = px.bar(tuples_main, \n",
    "             x='label', \n",
    "             y='percentage',\n",
    "             title='Related Subdivisions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On `aid_related`:\n",
    "\n",
    "- solution strongly based on code [here](https://stackoverflow.com/questions/47489554/plotly-deactivate-x-axis-sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode()\n",
    "tuples_main = udacourse2.fn_labels_report(dataset=df,\n",
    "                                         label_filter='related',\n",
    "                                         data_ret=True,\n",
    "                                         max_c=False)\n",
    "data = []\n",
    "\n",
    "data.append(go.Bar(name='aid',\n",
    "                   x=tuples_main['label'], \n",
    "                   y=tuples_main['percentage']))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title='Related Subcategories')\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode()\n",
    "tuples_main1 = udacourse2.fn_labels_report(dataset=df,\n",
    "                                           label_filter='aid',\n",
    "                                           data_ret=True,\n",
    "                                           max_c=False)\n",
    "\n",
    "tuples_main1 = tuples_main1.reindex([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,0])\n",
    "data = []\n",
    "\n",
    "data.append(go.Bar(name='aid',\n",
    "                   x=tuples_main1['label'], \n",
    "                   y=tuples_main1['percentage']))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title='Aid Related')\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_main2 = udacourse2.fn_labels_report(dataset=df,\n",
    "                                           label_filter='weather',\n",
    "                                           data_ret=True,\n",
    "                                           max_c=False)\n",
    "tuples_main2 = tuples_main2.reindex([0,1,2,4,5,3])\n",
    "\n",
    "data = []\n",
    "\n",
    "data.append(go.Bar(name='weather',\n",
    "                   x=tuples_main2['label'], \n",
    "                   y=tuples_main2['percentage']))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title='Weather Related')\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infrastructure\n",
    "\n",
    ">- my old code (don´t use it!)\n",
    ">- I cannot customize it so much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuples_main.index = ['buildings', 'transport', 'electricity', 'hospitals', 'shops', 'other_infrastructure']\n",
    "#tuples_main\n",
    "#fig = px.bar(tuples_main, x='label', y='percentage')\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode()\n",
    "tuples_main3 = udacourse2.fn_labels_report(dataset=df,\n",
    "                                           label_filter='infra',\n",
    "                                           data_ret=True,\n",
    "                                           max_c=False)\n",
    "tuples_main3 = tuples_main3.reindex([0,1,3,4,5,2])\n",
    "\n",
    "data = []\n",
    "\n",
    "data.append(go.Bar(name='infrastructure',\n",
    "                   x=tuples_main3['label'], \n",
    "                   y=tuples_main3['percentage']))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title=\"Infrastructure Related\")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting them together:\n",
    "\n",
    "- how to make this fancy graphic, at plotly [documentation](https://plotly.com/python/legend/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode()\n",
    "data = []\n",
    "\n",
    "data.append(go.Bar(name='aid',\n",
    "                   x=tuples_main1['label'], \n",
    "                   y=tuples_main1['percentage']))\n",
    "\n",
    "data.append(go.Bar(name='weather',\n",
    "                   x=tuples_main2['label'], \n",
    "                   y=tuples_main2['percentage']))\n",
    "\n",
    "data.append(go.Bar(name='infrastructure',\n",
    "                   x=tuples_main3['label'], \n",
    "                   y=tuples_main3['percentage']))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title='Subcategories from Related (aid, weather, infrastructure)')\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar') #, labels={'m', 'd', 'e'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related groups, with subgroups showed as colored layers:\n",
    "    \n",
    ">- observe that the **top** element of each bar is `other` subcategories\n",
    ">- as `aid_related` has a lot of elements on `other_aid` subcategorie, a massive block appears on the top of this bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode()\n",
    "tuples_main1 = tuples_main1.reset_index(drop=True)\n",
    "tuples_main1 = tuples_main1.drop(columns=['label', 'count'], axis=1)\n",
    "tuples_main1.columns = ['aid_related']\n",
    "\n",
    "tuples_main1 = tuples_main1.reset_index(drop=True)\n",
    "tuples_main2 = tuples_main2.drop(columns=['label', 'count'], axis=1)\n",
    "tuples_main2.columns = ['weather_related']\n",
    "\n",
    "tuples_main1 = tuples_main1.reset_index(drop=True)\n",
    "tuples_main3 = tuples_main3.drop(columns=['label', 'count'], axis=1)\n",
    "tuples_main3.columns = ['infrastructure_related']\n",
    "\n",
    "tuples_main_tot = pd.concat([tuples_main1, tuples_main2, tuples_main3], axis=1)\n",
    "\n",
    "data = []\n",
    "related_list = ['aid_related', 'weather_related', 'infrastructure_related']\n",
    "\n",
    "for i in range(0,tuples_main_tot.shape[0]):\n",
    "    data.append(go.Bar(name='related',\n",
    "                       x=related_list, \n",
    "                       y=tuples_main_tot.iloc[i]))\n",
    "\n",
    "layout = go.Layout(barmode='stack', \n",
    "                   xaxis=dict(type='category'),\n",
    "                   yaxis=dict(title='Percentage by category'),\n",
    "                   title=\"Related with Subcategories\")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_layout(showlegend=False)\n",
    "plotly.offline.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Some studies about labels\n",
    "\n",
    "*OK, I already know that **labels** are not **features**... but we may do some **critics** about the labels that exists for multi-classification*\n",
    "\n",
    "Strong candidates labels candidates for **ignoring**:\n",
    "    \n",
    ">- **related** (75.0%) $\\rightarrow$ too **weighty** and looks **meaningless**\n",
    ">- **child_alone** (empty) $\\rightarrow$ **impossible to train**, as it don´t have any valid element in the dataset\n",
    ">- **request** (17.1%), **offer** (0.4%) **direct_report** (19.4%) $\\rightarrow$ looks **meaningless**\n",
    "\n",
    "---\n",
    "\n",
    "#### Another viewpoint about these labels\n",
    "\n",
    "If we look at them more carefully, we can find a curious pattern on them\n",
    "\n",
    "These labels looks as they have a kind of hierarchy behind their shape, as:\n",
    "\n",
    "First **hierarchical** class: \n",
    "\n",
    ">- **related**\n",
    ">- **request**\n",
    ">- **offer**\n",
    ">- **direct_report**\n",
    "\n",
    "And then, **related** seems to have a **Second** hierarchical class\n",
    "\n",
    "Features for considering a training a classifier on **two layes**, or to **group** them all in main groups, as they are clearly **collinear**:\n",
    "\n",
    ">- **aid_related** $\\rightarrow$ groups aid calling (new things to add/ to do **after** the disaster)\n",
    ">>- **food**\n",
    ">>- **shelter**\n",
    ">>- **water**\n",
    ">>- **death**\n",
    ">>- **refugees**\n",
    ">>- **money**\n",
    ">>- **security**\n",
    ">>- **military**\n",
    ">>- **clothing**\n",
    ">>- **tools**\n",
    ">>- **missing_people**\n",
    ">>- **child_alone**\n",
    ">>- **search_and_rescue**\n",
    ">>- **medical_help**\n",
    ">>- **medical_products**\n",
    ">>- **aid_centers**\n",
    ">>- **other_aid**\n",
    ">- **weather_related** $\\rightarrow$ groups what was the main **cause** of the disaster\n",
    ">>- **earthquake**\n",
    ">>- **storm**\n",
    ">>- **floods**\n",
    ">>- **fire**\n",
    ">>- **cold**\n",
    ">>- **other_weather**\n",
    ">- **infrastructure_related** $\\rightarrow$ groups **heavy infra** that was probably dammaged during the disaster\n",
    ">>- **buildings**\n",
    ">>- **transport**\n",
    ">>- **hospitals**\n",
    ">>- **electricity**\n",
    ">>- **shops**\n",
    ">>- **other_infrastructure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s filter & count for one **subcategory**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['food'] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to have a union of two subcategories:\n",
    "\n",
    "- how to make multifilters [here](https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series)\n",
    "\n",
    "- first trying two categories + **OR** clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['food'] == 1) ^ (df['shelter'] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning into a more automatized mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"(df['food'] == 1) ^ (df['shelter'] == 1)\"\n",
    "\n",
    "df[eval(a)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to turn it into a function\n",
    "\n",
    "- `fn_cat_condenser` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_aid_related = aid_lst\n",
    "dataset = 'df'\n",
    "element = '1'\n",
    "opperator = '=='\n",
    "condition = '^'\n",
    "string = ''\n",
    "\n",
    "for item in cat_aid_related:\n",
    "    string = string + \"(\" + dataset + \"['\" + item + \"'] \" + opperator + \" \" + element + \")\" + \" \" + condition + \" \"\n",
    "    \n",
    "string[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tests for class `aid_related` \n",
    "\n",
    "Counting for the main class **aid_related**:\n",
    "    \n",
    "- you can see that **aid_related** have more rows registered than all subclasses counted together\n",
    "\n",
    "- this is not about training a **Machine Learning**, this is about **database data consistency**\n",
    "\n",
    "*Just think in this way: if something is labelled as **aid_related**, so every data under it may be contained by **aid_related**. So next step, we need to correct this thing, turning **aid_related = 1** for all of them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aid_main = df[df['aid_related'] == 1]\n",
    "df_aid_main.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for **main**, without any **sub-category** registered:\n",
    "\n",
    "- this is **empty data** as the main category is checked, but the subcategory is not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_cat_condenser(subset='aid', opperation='main_not_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_not_sub = df[eval(udacourse2.fn_cat_condenser(subset='aid', \n",
    "                                                   name='df',\n",
    "                                                   opperation='main_not_sub')[0])]\n",
    "main_not_sub.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function for grouping all **subclasses**\n",
    "\n",
    "- counting for all subclasses of **aid_related*\n",
    "\n",
    "- filtering for all **subcategories** with any register:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_cat_condenser(subset='aid', opperation='all_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aid_subsets = df[eval(udacourse2.fn_cat_condenser(subset='aid',\n",
    "                                                      name='df',\n",
    "                                                      opperation='all_sub')[0])]\n",
    "all_aid_subsets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for **all empty** subcategories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_cat_condenser(subset='aid', opperation='sub_not_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_not_main = df[eval(udacourse2.fn_cat_condenser(subset='aid',\n",
    "                                                   name='df',\n",
    "                                                   opperation='empty_sub')[0])]\n",
    "sub_not_main.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Inconsistency detected\n",
    "\n",
    "1. Just consider that our **labels** have a kind of **hierarchical structure**\n",
    "\n",
    "2. Before running a Machine Learning **Classifier**, we can procede some **mechanical opperations** for correcting database **data inconsistencies**\n",
    "\n",
    "3. The theoretical support for this opperations resides on **database theory** (e.g. if something is a subclass of other thing, so, the class must be setted on for each of the rows that are setted for at least one item of the subclass) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for **empty main** with any **subcategory** registered:\n",
    "\n",
    ">- this is a **database inconsistency** \n",
    ">- as if a **subgroup** is valid, so the **main** group must be valid too!\n",
    ">- about database **normal forms**, please read [here](https://www.guru99.com/database-normalization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_cat_condenser(subset='aid', opperation='sub_not_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_not_main = df[eval(udacourse2.fn_cat_condenser(subset='aid',\n",
    "                                                   name='df',\n",
    "                                                   opperation='sub_not_main')[0])]\n",
    "sub_not_main.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting our **data inconsistency**:\n",
    "\n",
    ">- considering `opp` is only a **filter** for columns that have any value in sublabels\n",
    ">- `df[eval(opp[0])]` gives our dataset rows for this filter\n",
    ">- `df[eval(opp[0])].index` is only about the address to find these rows\n",
    ">- finally, we need to correct `opp[1]]`, that is the name of the column, by the value `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp = udacourse2.fn_cat_condenser(subset='aid',\n",
    "                                  name='df',\n",
    "                                  opperation='sub_not_main')\n",
    "\n",
    "#df[eval(opp[0])]\n",
    "#df.loc[df[eval(opp[0])].index, opp[1]] = 1 #I don´t want to correct it now!\n",
    "\n",
    "sub_not_main = df[eval(udacourse2.fn_cat_condenser(subset='aid', opperation='sub_not_main')[0])]\n",
    "sub_not_main.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning this into a function\n",
    "\n",
    "- function `fn_croup_check` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aid = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='aid',\n",
    "                                   correct=True, \n",
    "                                   shrink=True, \n",
    "                                   shorten=True, \n",
    "                                   verbose=True)\n",
    "df_aid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### About rows:\n",
    "    \n",
    "- We have 40 columns, and some rows that cannot be used to **train** any model (all their features are blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blank labels:\n",
    "    \n",
    "- it's not a good idea training with this data;\n",
    "\n",
    "- they have **zero** classification (no labels at all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['if_blank']].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels with some content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['if_blank']].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test area (can be removed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = 'c://host/pyprog/project/udacourse/categories.csv'\n",
    "full_path = 'c://host/pyprog/project/udacourse/messages.csv'\n",
    "last_one = full_path.rfind('/')\n",
    "full_path[last_one+1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.index.name = 'categories'\n",
    "categories.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uma = 0.70\n",
    "duas = 0.305\n",
    "primeira = 0.35 #0.51\n",
    "segunda = 0.67 #0.74\n",
    "\n",
    "delta = ((uma - duas) * primeira) + (duas * segunda)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uma = 0.70\n",
    "duas = 0.305\n",
    "primeira = 0.70\n",
    "segunda = 0.88\n",
    "\n",
    "original = ((uma - duas) * primeira) + (duas * segunda)\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeira = 0.51\n",
    "segunda = 0.74\n",
    "\n",
    "alfa = ((uma - duas) * primeira) + (duas * segunda)\n",
    "alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_teste(dataset):\n",
    "    #print(dataset.iloc[0])\n",
    "    print(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_teste(dataset=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuples, columns = ['label', 'count', 'percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/Documents/Python/CKANMay.csv')\n",
    "sd = df.nlargest(3,'Views')\n",
    "fd = sd.sort_values(by='Views', ascending = False)\n",
    "\n",
    "\n",
    "my_data = [go.Bar( x = fd.Views, y = fd.Publisher, orientation = 'h')]\n",
    "my_layout = ({\"title\": \"Most popular publishers\",\n",
    "                       \"yaxis\": {\"title\":\"Publisher\"},\n",
    "                       \"xaxis\": {\"title\":\"Views\"},\n",
    "                       \"showlegend\": False})\n",
    "\n",
    "fig = go.Figure(data = my_data, layout = my_layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
