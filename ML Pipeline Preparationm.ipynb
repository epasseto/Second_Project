{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "### 1. Import libraries and load data from database.\n",
    ">- Import Python libraries\n",
    ">- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    ">- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#measuring time and making basic math\n",
    "from time import time\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "import statistics\n",
    "\n",
    "#my own ETL pipeline\n",
    "#import process_data as pr\n",
    "\n",
    "#dealing with datasets and showing content\n",
    "import pandas as pd\n",
    "#import pprint as pp\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#natural language toolkit\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#REGEX toolkit\n",
    "import re\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier #need MOClassifier!\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Machine Learning Metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#pickling tool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to use NLTK, I took the following error:\n",
    "\n",
    "- the point is - it´s not only about installing a library\n",
    "\n",
    "- you need to install de supporting dictionnaries for doing the tasks\n",
    "\n",
    "- this can be solved quite easilly (in hope that I will find a Portuguese-Brazil dictionnary when I will need to put it in practic in my work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LookupError: \n",
    "    **********************************************************************\n",
    "      Resource stopwords not found.\n",
    "      Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "      >>> import nltk\n",
    "      >>> nltk.download('stopwords')\n",
    "  \n",
    "      For more information see: https://www.nltk.org/data.html\n",
    "\n",
    "      Attempted to load corpora/stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LookupError: \n",
    "    **********************************************************************\n",
    "    Resource stopwords not found.\n",
    "    Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LookupError: \n",
    "    **********************************************************************\n",
    "    Resource wordnet not found.\n",
    "    Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from database\n",
    "#setting NullPool prevents a pool, so it is easy to close the database connection\n",
    "#in our case, the DB is so simple, that it looks the best choice\n",
    "#SLQAlchemy documentation\n",
    "#https://docs.sqlalchemy.org/en/14/core/reflection.html\n",
    "engine = create_engine('sqlite:///Messages.db', poolclass=pool.NullPool) #, echo=True)\n",
    "\n",
    "#retrieving tables names from my DB\n",
    "#https://stackoverflow.com/questions/6473925/sqlalchemy-getting-a-list-of-tables\n",
    "inspector = inspect(engine)\n",
    "print('existing tables in my SQLite database:', inspector.get_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my target is Messages table, so I reed this table as a Pandas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing MySQL to Pandas\n",
    "#https://stackoverflow.com/questions/37730243/importing-data-from-a-mysql-database-into-a-pandas-data-frame-including-column-n/37730334\n",
    "#connection_str = 'mysql+pymysql://mysql_user:mysql_password@mysql_host/mysql_db'\n",
    "#connection = create_engine(connection_str)\n",
    "\n",
    "connection = engine.connect()\n",
    "df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "connection.close()\n",
    "\n",
    "df.name = 'df'\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting in X and Y datasets:\n",
    "\n",
    "- X is the **Message** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Y is the **Classification** labels\n",
    "\n",
    "- I excluded all my columns that don´t make sense as labels to classify our message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[df.columns[4:]]\n",
    "Y.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_text = X.iloc[0]\n",
    "msg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let´s insert some noise to see if it is filtering well\n",
    "msg_text = \"Weather update01 - a 00cold-front from Cuba's that could pass over Haiti' today\"\n",
    "low_text = msg_text.lower()\n",
    "\n",
    "#I need to take only valid words\n",
    "#a basic one (very common in Regex courses classes)\n",
    "gex_text = re.sub(r'[^a-zA-Z]', ' ', low_text)\n",
    "\n",
    "#other tryed sollutions from several sources\n",
    "#re.sub(r'^\\b[^a-zA-Z]\\b', ' ', low_text)\n",
    "#re.sub(r'^/[^a-zA-Z ]/g', ' ', low_text)\n",
    "#re.sub(r'^/[^a-zA-Z0-9 ]/g', ' ', low_text)\n",
    "\n",
    "gex_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found this [here](https://stackoverflow.com/questions/1751301/regex-match-entire-words-only)\n",
    "\n",
    "- '-' passed away, so it´s not so nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'^/\\b($word)\\b/i', ' ', low_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'^\\b[a-zA-Z]{3}\\b', ' ', low_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'^[a-zA-Z]{3}$', ' ', low_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_words = word_tokenize(gex_text)\n",
    "col_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnuseful = stopwords.words(\"english\")\n",
    "relevant_words = [word for word in col_words if word not in unnuseful]\n",
    "relevant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed a lot of geographic references. I think they will not be so useful for us. Let´s try to remove them too...\n",
    "\n",
    "References for City at NLKT [here](https://stackoverflow.com/questions/37025872/unable-to-import-city-database-dataset-from-nltk-data-in-anaconda-spyder-windows?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sem.chat80 as ct #.sql_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LookupError: \n",
    "**********************************************************************\n",
    "  Resource city_database not found.\n",
    "  Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('city_database')\n",
    "  \n",
    "  For more information see: https://www.nltk.org/data.html\n",
    "\n",
    "  Attempted to load corpora/city_database/city.db\n",
    "\n",
    "  Searched in:\n",
    "    - 'C:\\\\Users\\\\epass/nltk_data'\n",
    "    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n",
    "    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n",
    "    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n",
    "    - 'C:\\\\Users\\\\epass\\\\AppData\\\\Roaming\\\\nltk_data'\n",
    "    - 'C:\\\\nltk_data'\n",
    "    - 'D:\\\\nltk_data'\n",
    "    - 'E:\\\\nltk_data'\n",
    "**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('city_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {\n",
    "    country:city for city, country in ct.sql_query(\n",
    "        \"corpora/city_database/city.db\",\n",
    "        \"SELECT City, Country FROM city_table\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look nice (and lower cased):\n",
    "    \n",
    "- observe possible errors with composite names, like united_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't find Haiti:\n",
    "\n",
    "- countries list is not complete!\n",
    "\n",
    "- it gaves `KeyError: 'haiti'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries['haiti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nogeo_words = [word for word in relevant_words if word not in countries]\n",
    "nogeo_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortatelly, it´s only a **demo**! We need something better for our project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cities = pd.read_csv('cities15000.txt', sep=';')\n",
    "df_cities = pd.read_csv('cities15000.txt', sep='\\t', header=None)\n",
    "df_cities_15000 = df_cities[[1, 17]]\n",
    "df_cities_15000.columns = ['City', 'Region']\n",
    "df_cities_15000.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried this [here](https://data.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000%40public/information/?disjunctive.cou_name_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "found country names at Github [here](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv)\n",
    "\n",
    "- a small trick and we have our own coutries list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries = pd.read_csv('all.csv')\n",
    "df_countries = df_countries['name'].apply(lambda x: x.lower())\n",
    "countries = df_countries.tolist()\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can elliminate (perhaps not the whole) a lot of names of countries. In our case, the produce noise on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nogeo_words = [word for word in relevant_words if word not in countries]\n",
    "nogeo_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First test:\n",
    "    \n",
    "- over the first message only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Weather update - a cold front from Cuba that could pass over Haiti'\n",
    "tokens = udacourse2.fn_tokenize_fast(msg_text, \n",
    "                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Weather update - a cold front from Cuba that could pass over Haiti'\n",
    "tokens = udacourse2.fn_tokenize(msg_text, \n",
    "                                lemmatize=True, \n",
    "                                rem_city=True, \n",
    "                                agg_words=True,\n",
    "                                rem_noise=True,\n",
    "                                elm_short=3,\n",
    "                                verbose=True)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It´s not so cool, some noise is still appearing in lemmatized words:\n",
    "    \n",
    "- an \"l\" was found, as in **French words**, like *l'orange*;\n",
    "\n",
    "- my **City** filter needs a lot of improving, as it didn´t filter avenues and so many other **geographic** references;\n",
    "\n",
    "- it passed a lot of unnuseful **two** or less letters words, as **u**, **st**;\n",
    "\n",
    "- a lot of noisy words as **help**, **thanks**, **please** were found;\n",
    "\n",
    "- there are several words **repetition** in some messages, like ['river', ... 'river', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic test call\n",
    "\n",
    "- only for the first 50 messages, verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_start = time()\n",
    "\n",
    "i = 0\n",
    "for message in X:\n",
    "    out = udacourse2.fn_tokenize_fast(message, \n",
    "                                      verbose=True)\n",
    "    i += 1\n",
    "    if i > 200: #it´s only for test, you can adjust it!\n",
    "        break\n",
    "\n",
    "b_spent = time() - b_start\n",
    "print('process time:{:.0f} seconds'.format(b_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_start = time()\n",
    "\n",
    "i = 0\n",
    "for message in X:\n",
    "    print(message)\n",
    "    out = udacourse2.fn_tokenize(message, \n",
    "                                 lemmatize=True, \n",
    "                                 rem_city=True, \n",
    "                                 agg_words=True,\n",
    "                                 rem_noise=True,\n",
    "                                 elm_short=3,\n",
    "                                 great_noisy=True,\n",
    "                                 verbose=True)\n",
    "    print(out)\n",
    "    print()\n",
    "    i += 1\n",
    "    if i > 20: #it´s only for test, you can adjust it!\n",
    "        break\n",
    "\n",
    "b_spent = time() - b_start\n",
    "print('process time:{:.4f} seconds'.format(b_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don´t try it! (complete tokenizer)\n",
    "\n",
    "- it´s a slow test! (takes like 221 seconds to tokenize all the dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_start = time()\n",
    "\n",
    "#X_tokens = X.apply(lambda x: udacourse2.fn_tokenize(x, \n",
    "#                                                    lemmatize=True, \n",
    "#                                                    rem_city=True, \n",
    "#                                                    agg_words=True,\n",
    "#                                                    rem_noise=True,\n",
    "#                                                    elm_short=3,\n",
    "#                                                    great_noisy=True,\n",
    "#                                                    verbose=False))\n",
    "\n",
    "#b_spent = time() - b_start\n",
    "#print('process time:{:.0f} seconds'.format(b_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it´s a bit faster test (it takes 46 seconds to run)\n",
    "\n",
    "- the secret is that it loops only one time for row, as it condenses all the filters into one loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_start = time()\n",
    "\n",
    "X_tokens = X.apply(lambda x: udacourse2.fn_tokenize_fast(x, \n",
    "                                                         verbose=False))\n",
    "\n",
    "b_spent = time() - b_start\n",
    "print('process time:{:.0f} seconds'.format(b_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a **series** with all my tokenized messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I can filter it for rows that have an **empty list**:\n",
    "    \n",
    "- solution found [here](https://stackoverflow.com/questions/29100380/remove-empty-lists-in-pandas-series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens[X_tokens.str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2 = X_tokens[X_tokens.str.len() > 0]\n",
    "ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_start = time()\n",
    "\n",
    "dic_tokens = udacourse2.fn_subcount_lists(column=X_tokens, \n",
    "                                          verbose=False)\n",
    "\n",
    "b_spent = time() - b_start\n",
    "print('process time:{:.0f} seconds'.format(b_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted dictionnary [here](https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_tokens\n",
    "\n",
    "d_tokens = dic_tokens['elements']\n",
    "t_sorted = sorted(d_tokens.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "if t_sorted:\n",
    "    print('data processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted list of tuples of most counted tokens:\n",
    "\n",
    "- filtering the more counted 300 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sorted[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the **tokenize** function just to absorve less meaningful tokens to discard:\n",
    "    \n",
    "- **ver 1.2** update: tokenizer function created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_noisy = ['people', 'help', 'need', 'said', 'country', 'government', 'one', 'year', 'good', 'day',\n",
    "    'two', 'get', 'message', 'many', 'region', 'city', 'province', 'road', 'district', 'including', 'time',\n",
    "    'new', 'still', 'due', 'local', 'part', 'problem', 'may', 'take', 'come', 'effort', 'note', 'around',\n",
    "    'person', 'lot', 'already', 'situation', 'see', 'response', 'even', 'reported', 'caused', 'village', 'bit',\n",
    "    'made', 'way', 'across', 'west', 'never', 'southern', 'january', 'least', 'zone', 'small', 'next', 'little',\n",
    "    'four', 'must', 'non', 'used', 'five', 'wfp', 'however', 'com', 'set', 'every', 'think', 'item', 'yet', \n",
    "    'carrefour', 'asking', 'ask', 'site', 'line', 'put', 'unicef', 'got', 'east', 'june', 'got', 'ministry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Older atempt to clear tokens\n",
    "\n",
    "Tried to isolate some words that I think are noisy, for exclusion:\n",
    "    \n",
    "- general geographic references, as **area** and **village**;\n",
    "\n",
    "- social communication words, as **thanks** and **please**;\n",
    "\n",
    "- religious ways to talk, as **pray**\n",
    "\n",
    "- unmeaningful words, as **thing** and **like**\n",
    "\n",
    "- visually filtered some words that I think don´t aggregate too much to the **Machine Learning**\n",
    "\n",
    "- just think about - you prefer your **IA** trained for 'thanks' or for 'hurricane'?\n",
    "\n",
    "- really I´m not 100% sure about these words, buy my **tokenize** function can enable and disable this list, and re-train the machine, and see if the performance increase or decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhelpful_words = ['thank', 'thanks', 'god', 'fine', 'number', 'area', 'let', 'stop', 'know', 'going', 'thing',\n",
    "    'would', 'hello', 'say', 'neither', 'right', 'asap', 'near', 'want', 'also', 'like', 'since', 'grace', \n",
    "    'congratulate', 'situated', 'tell', 'almost', 'hyme', 'sainte', 'croix', 'ville', 'street', 'valley', 'section',\n",
    "    'carnaval', 'rap', 'cry', 'location', 'ples', 'bless', 'entire', 'specially',  'sorry', 'saint', 'village', \n",
    "    'located', 'palace', 'might', 'given']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing **elliminate duplicates**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['addon', 'place', 'addon']\n",
    "test = list(set(test))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing **elliminate short words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 3\n",
    "list2 = []\n",
    "test2 = ['addon', 'l', 'us', 'place']\n",
    "\n",
    "for word in test2:\n",
    "    if len(word) < min:\n",
    "        print('elliminate:', word)\n",
    "    else: \n",
    "        list2.append(word)\n",
    "    \n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution [here](https://stackoverflow.com/questions/3501382/checking-whether-a-variable-is-an-integer-or-not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(min, int):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have two **Tokenizer** functions:\n",
    "\n",
    "- `fn_tokenize` $\\rightarrow$ it allows to test each individual methods, and contains all the methods described, but a bit slow, as it iterates all the words again for each method\n",
    "\n",
    "- `fn_tokenize_fast` $\\rightarrow$ it is a **boosted** version, with only one iteration, for running faster, but you cannot set each method individually for more accurate test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### A small review over each item for our first machine learning pipelines\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "Feature Extraction from SKlearn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "\"Convert a collection of text documents to a matrix of token counts\"\n",
    "\n",
    "- we are looking for **tokens** that will be turned into **vectors** in a Machine Learning Model;\n",
    "\n",
    "- they are represented as **scalars** in a **matrix**, that indicates the scale of each one of these tokens.\n",
    "\n",
    "\"This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\"\n",
    "\n",
    "- normally matrix representations of the natural reallity are a bit **sparse**\n",
    "\n",
    "- in this case, to save some memory, they indicate a use of a propper representation\n",
    "\n",
    "\"If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\"\n",
    "\n",
    "- me already made it, drastically reducing the **variability** of terms\n",
    "\n",
    "- it its represented by our **fn_tokenizer**\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "TF-IDF from SKlearn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "\n",
    "- **tf** is about **term frequency** and;\n",
    "\n",
    "- **idf** is about **inverse document frequency**.\n",
    "\n",
    "\"Transform a count matrix to a normalized tf or tf-idf representation\"\n",
    "\n",
    "- it means that it basically **normalizes** the count matrix\n",
    "\n",
    "*Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.*\n",
    "\n",
    "- it takes term-frequency and it **rescales** it by the gereral document-frequency\n",
    "\n",
    "*The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.*\n",
    "\n",
    "- the idea is to not weight too much a **noisy** and very frequent word\n",
    "\n",
    "- we tried to \"manually\" elliminate some of the **noisy** words, but as the number of tokens is too high, it´s quite impossible to make a good job\n",
    "\n",
    "#### Training a Machine Learning\n",
    "\n",
    "As we have **labels**, a good strategy is to use **supervised learning**\n",
    "\n",
    "- we could try to kind of make **clusters** of messages, using **unsupervised learning**, or try some strategy on **semi-supervised learning**, as we have some of the messages (40) that don´t have any classification;\n",
    "\n",
    "- the most obvious way is to train a **Classifier**;\n",
    "\n",
    "- as we have multiple labels, a **Multi Target Classifier** seems to be the better choice.\n",
    "\n",
    "Multi target classification [here](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html)\n",
    "\n",
    "\"This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification\"\n",
    "\n",
    "- OK, we will be basically using **slices** of train for each feature, as we don´t have so much **Machines** that are natively supporting multi-target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Prepare the data\n",
    "\n",
    "Make the lasts opperations for preparing the dataset for training on **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **training** data, it is a **data inconsistency** if you consider that all the labels are blank\n",
    "\n",
    "- so we have 6,317 rows that we need to **remove** before **training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['if_blank'] == 0]\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying if removal was complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "    print('removal complete!')\n",
    "else:\n",
    "    raise Exception('something went wrong with rows removal before training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1.3** update: **pre-tokenizer** (a premature tokenization strategy) created, for removing **untrainable rows**\n",
    "\n",
    "What is this **crazy thing** over here? \n",
    "\n",
    ">- I created a **provisory** column, and **tokenizing** it\n",
    ">- Why I need it for now? Just for removing rows that are **impossible to train**\n",
    ">- After tokenization, if I get a **empty list**, I need to remove this row before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "try:\n",
    "    df = df.drop('tokenized', axis=1)\n",
    "except KeyError:\n",
    "    print('OK')\n",
    "\n",
    "#inserting a provisory column\n",
    "df.insert(1, 'tokenized', np.nan)\n",
    "\n",
    "#tokenizing over the provisory\n",
    "df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "\n",
    "#removing NaN over provisory (if istill exist)\n",
    "df = df[df['tokenized'].notnull()]\n",
    "\n",
    "spent = time() - start\n",
    "print('process time:{:.0f} seconds'.format(spent))\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering empy lists on `provisory`, found [here](https://stackoverflow.com/questions/42964724/pandas-filter-out-column-values-containing-empty-list)\n",
    "\n",
    "**Version 1.4** update: could absorb **pre-tokenized** column as a input for **Machine Learning Classifier**, saving time!\n",
    "\n",
    "And another **crazy thing**, I regret about removing `provisory` tokenized column:\n",
    "\n",
    ">- why? Just because I already **trained** my **X** subdataset, and I will not need to do it later!\n",
    ">- and if I make the thing **wizely**, I will accelerate the pipeline process, as I already made the hard job for the **CountVectorized**\n",
    ">- it will also faccilitate to **train** diverse Classifiers, as I save a lot of individual processing, making it **early** in my process!\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.21** update: for preventing **pipeline leakage** using Picke, I modified `train_data.py` for having pre_tokenization preprocessing as optional. For more details see reference[here](https://rebeccabilbro.github.io/module-main-has-no-attribute/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "print('found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "\n",
    "#I will not drop it anymore!\n",
    "#try:\n",
    "#    df = df.drop('provisory', axis=1)\n",
    "#except KeyError:\n",
    "#    print('OK')\n",
    "\n",
    "#Instead, I will drop 'message' column\n",
    "try:\n",
    "    df = df.drop('message', axis=1)\n",
    "except KeyError:\n",
    "    print('OK')\n",
    "\n",
    "print('now I have {} rows to train'.format(df.shape[0]))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Database data inconsistency fix\n",
    "\n",
    "**Version 1.5** update: added **hierarchical structure** on labels, for checking and correcting unfilled classes that already have at least one subclass alredy filled\n",
    "\n",
    "A **more advanced** issue about these data\n",
    "\n",
    "A more detailed explanation, you can found at the file `ETL Pipeline Preparatione.ipynb`\n",
    "\n",
    "The fact is: \n",
    "\n",
    ">- these labels are not **chaotic** as we initially think they are\n",
    ">- looking with care, we can see a very clear **hierarchic structure** on them\n",
    ">- it they are really hierarchized, so, we can verify them for **data inconsistencies**, using **database fundamentals**\n",
    "\n",
    "---\n",
    "\n",
    "#### Another viewpoint about these labels\n",
    "\n",
    "If we look at them more carefully, we can find a curious pattern on them\n",
    "\n",
    "These labels looks as they have a kind of hierarchy behind their shape, as:\n",
    "\n",
    "First **hierarchical** class: \n",
    "\n",
    ">- **related**\n",
    ">- **request**\n",
    ">- **offer**\n",
    ">- **direct_report**\n",
    "\n",
    "And then, **related** seems to have a **Second** hierarchical class\n",
    "\n",
    "Features for considering a training a classifier on **two layes**, or to **group** them all in main groups, as they are clearly **collinear**:\n",
    "\n",
    ">- **aid_related** $\\rightarrow$ groups aid calling (new things to add/ to do **after** the disaster)\n",
    ">>- **food**\n",
    ">>- **shelter**\n",
    ">>- **water**\n",
    ">>- **death**\n",
    ">>- **refugees**\n",
    ">>- **money**\n",
    ">>- **security**\n",
    ">>- **military**\n",
    ">>- **clothing**\n",
    ">>- **tools**\n",
    ">>- **missing_people**\n",
    ">>- **child_alone**\n",
    ">>- **search_and_rescue**\n",
    ">>- **medical_help**\n",
    ">>- **medical_products**\n",
    ">>- **aid_centers**\n",
    ">>- **other_aid**\n",
    ">- **weather_related** $\\rightarrow$ groups what was the main **cause** of the disaster\n",
    ">>- **earthquake**\n",
    ">>- **storm**\n",
    ">>- **floods**\n",
    ">>- **fire**\n",
    ">>- **cold**\n",
    ">>- **other_weather**\n",
    ">- **infrastructure_related** $\\rightarrow$ groups **heavy infra** that was probably dammaged during the disaster\n",
    ">>- **buildings**\n",
    ">>- **transport**\n",
    ">>- **hospitals**\n",
    ">>- **electricity**\n",
    ">>- **shops**\n",
    ">>- **other_infrastructure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a correction for **database data consistency**:\n",
    "\n",
    ">- using the function that I already created (see: `ETL Pipeline Preparatione.ipynb`)\n",
    ">- the idea is when at least some element of a **subcategory** is filled for one **category**, it is expected that the **category** was filled too\n",
    ">- this is valido for the main category **related** too!\n",
    "\n",
    "*This is only one more **advanced step** for **data preparation**, as it involves only a mechanic and automatized correction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correction for aid_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='aid',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for weather_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='wtr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for infrastrucutre_related\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='ifr',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "#correction for related(considering that the earlier were already corrected)\n",
    "df = udacourse2.fn_group_check(dataset=df,\n",
    "                               subset='main',\n",
    "                               correct=True, \n",
    "                               shrink=False, \n",
    "                               shorten=False, \n",
    "                               verbose=True)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Break the data\n",
    "\n",
    "Break the dataset into the **training columns** and **labels** (if it have **multilabels**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is the **Training Text Column**:\n",
    "    \n",
    "- if I observe the potential training data really well, I could `genre` column as training data too!\n",
    "\n",
    "- or I can use also `related`, `request`, `offer` columns for training `aid_related` data\n",
    "\n",
    "*A discussion of how much these **Label** columns are **hierarchically defined** is made laterly in this notebook*\n",
    "\n",
    "---\n",
    "\n",
    "For this moment, I am using only `message` as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokenized']\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is constituted by the **Classification Labels**\n",
    "\n",
    "**Version 1.6** update: removed `related` column from the Labels dataset. Why? Because when I go to statistics after training the **Machine Learning Classifier**, it turns allways at `1`. So, sometimes this column (like in Adaboost) is causing problems when training our Classifier, and adding nothing to the model\n",
    "\n",
    ">- was: `y = df[df.columns[4:]]`\n",
    ">- now: `y = df[df.columns[5:]]`\n",
    "\n",
    "**Version 1.7** update: removed from training columns that contains **only zeroes** on labels. Why? Just because they are **impossible to train** on our Classifier!, so they add nothing to the model\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.19** update: **not** removing anymore any column from the Labels dataset. For accomplish criteria for project approving, it is needed to train **exactly** 36 labels. I know that these ones cannot be trained, or train so poorly with the data that was provided. But it is only about obeying the **requisites** for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[df.columns[4:]]\n",
    "#y = df[df.columns[5:]] #uncheck this if you want to elliminate related column\n",
    "\n",
    "#uncheck this if you want to elliminate untrainable columns (everything is zero)\n",
    "#remove_lst = []\n",
    "\n",
    "#for column in y.columns:\n",
    "#    col = y[column]\n",
    "#    if (col == 0).all():\n",
    "#        print('*{} -> only zeroes training column!'.format(column))\n",
    "#        remove_lst.append(column)\n",
    "#    else:\n",
    "#        #print('*{} -> column OK'.format(column))\n",
    "#        pass\n",
    "#print(remove_lst)\n",
    "\n",
    "#y = y.drop(remove_lst, axis=1)\n",
    "\n",
    "verbose=True\n",
    "\n",
    "if y.shape[1] == 36:\n",
    "    if verbose:\n",
    "        print('y dataset has 36 labels')\n",
    "else:\n",
    "    raise Exception('something went wrong, dataset has {} labels instead of 36'.format(y.shape[1]))\n",
    "y.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Split the data \n",
    "\n",
    "Into **Train** and **Test** subdatasets\n",
    "\n",
    ">- let´s start it with **20%** of test data\n",
    ">- I am not using **random_state** settings (and why **42**? I personally think it is about a reference to the book **The Hitchhicker´s Guide to de Galaxy**, from Douglas Adams!\n",
    "\n",
    "**Version 1.8** update: now I am using **random_state** parameter, so I can compare exactly the same thing, when using randomized processes, for ensuring the same results for each function call\n",
    "\n",
    "---\n",
    "\n",
    "**Future** possible updates:\n",
    "\n",
    ">- I can test/train using other parameters for test_size, like **0.25** and see if it interfers so much\n",
    ">- I can try to do **bootstrap** and see if I can plot a good **normalization** curve for it!\n",
    "\n",
    "**NEW Future** possible update:\n",
    "\n",
    ">- I could use **Cross Validation** in order to use all my data for training!\n",
    ">- **Warning** there are some papers saying that to take care about using **Cross Validation** on Model Training. The reason is, it may let **data leakage** from your **train** to your **test** dataset, masking the real power of your model!\n",
    ">- so I need to **study more** about that before trying to implement it in Python\n",
    ">- the discussion about \"data leakage\" when using cross validation strategies when **fitting** data is [here](https://stackoverflow.com/questions/56129726/fitting-model-when-using-cross-validation-to-evaluate-performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split makes randomization, so random_state parameter was set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Choose your first Classifier \n",
    "\n",
    "- and build a **Pipeline** for it\n",
    "\n",
    "Each Pipeline is a Python Object that can be called for **methods**, as **fit()**\n",
    "\n",
    "---\n",
    "\n",
    "What **Classifier** to choose?\n",
    "\n",
    "- **Towards Data Science** give us some tips [here](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)\n",
    "\n",
    "---\n",
    "\n",
    "Start with a **Naïve Bayes** (NB)\n",
    "\n",
    "`clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)`\n",
    "\n",
    "In a Pipeline way (pipeline documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):\n",
    "\n",
    ">- I had some issues with `CountVectorizer`, but could clear it using Stack Overflow [here](https://stackoverflow.com/questions/32674380/countvectorizer-vocabulary-wasnt-fitted)\n",
    ">- should I use `CountVectorizer(tokenizer=udacourse2.fn_tokenize_fast)`?... but I will **not**!\n",
    ">- why? Just because I already proceeded with **tokenization** in a earlier step\n",
    ">- so, how to overpass this hellish `tokenizer=...` parameter?\n",
    ">- I found a clever solution [here](https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer)\n",
    ">- so, I prepared a **dummy** function to overpass the tokenizer over **CountVertorizer**\n",
    "\n",
    "First I tried to set Classifier as **MultinomialNB()**, and it crashes:\n",
    "\n",
    ">- only **one** Label to be trained was expected, and there were 36 Labels!;\n",
    ">- reading the documentation for SKlearn, it turned clear that it is necessary (if your Classifier algorithm was not originally built for **multicriteria**, to run it **n** times, one for each label\n",
    ">- so it is necessary to include it our pipeline, using `MultiOutputClassifier()` transformer\n",
    "\n",
    "*And... it looks pretty **fast** to train, not? What is the secret? We are **bypassing** the tokenizer and preprecessor, as we **already made** it at the dataset!*\n",
    "\n",
    "*Another thing, we are not using the **whole** dataset... it´s just about a little **issue** we have, as there are a lot of **missing labels** at the dataset! And for me, it will **distort** our training! (lately I will compare the results with traning the **raw** dataset)*\n",
    "\n",
    "**Naïve Bayes** is known as a very **fast** method:\n",
    "\n",
    ">- but it is also known as being not so **accurate**\n",
    ">- and it have so **few** parameters for a later refinement\n",
    "\n",
    "I could reach Model Accuracy of **92.2**, after **.58** seconds for fitting the Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#Naïve Bayes classifier pipeline - no randomization involved\n",
    "pipeline_mbnb = Pipeline([('vect', CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf',  MultiOutputClassifier(MultinomialNB()))])\n",
    "                          #('clf', MultinomialNB())]) #<-my terrible mistake!\n",
    "#remembering:\n",
    "#CountVectorizer -> makes the count for tokenized vectors\n",
    "#TfidTransformer -> makes the weight \"normalization\" for word occurences\n",
    "#MultinomialNB -> is my Classifier\n",
    "\n",
    "#fit text_clf (our first Classifier model)\n",
    "pipeline_mbnb.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "print('NAÏVE BAYES - process time: {:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want, I can see the parameters for my **Pipeline**, using this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_mbnb.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Run metrics for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting using **Naïve Bayes** Classifier\n",
    "\n",
    "And I took this **weird** Error Message:\n",
    "\n",
    "\"**UndefinedMetricWarning:**\" \n",
    "\n",
    ">- \"Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples\"\n",
    ">- \"Use `zero_division` parameter to control this behavior\"\n",
    "\n",
    "And searching, I found this explanation [here](https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi)\n",
    "\n",
    ">- it is not an **weird error** at all. Some labels could´t be predicted when running the Classifier\n",
    ">- so the report don´t know how to handle them\n",
    "\n",
    "\"What you can do, is decide that you are not interested in the scores of labels that were not predicted, and then explicitly specify the labels you are interested in (which are labels that were predicted at least once):\"\n",
    "\n",
    "`metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))`\n",
    "\n",
    "#### Dealing with this issue\n",
    "\n",
    "**First**, I altered my function `fn_plot_scores` for not allowing comparisons over an empty (**not trained**) column, as `y_pred`\n",
    "\n",
    "And to check if all predicted values are **zeroes** [here](https://stackoverflow.com/questions/48570797/check-if-pandas-column-contains-all-zeros)\n",
    "\n",
    "And I was using in my function a **general** calculus for Accuracy. The problem is: **zeroes** for **zeroes** result a **1** accuracy, distorting my actual Accuracy, for a better (**unreal**) higher value:\n",
    "\n",
    ">- so, for general model Accuracy, I cannot use this `accuracy = (y_pred == y_test.values).mean()`\n",
    ">- using instead `f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))`\n",
    "\n",
    "**Version 1.9** updated: created my own customized function for showing metrics\n",
    "\n",
    "**Version 1.15** updated: improved my customized function for other metrics\n",
    "\n",
    ">- I was using the mean F1 Score as \"Model Precision\" and that seems a bit **silly**, as there were other metrics\n",
    ">- I could find a better material At SkLearn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    ">- for example, as we are using binary labels, and the most important one is the \"1\", label, we can set is in the parameters as `average='binary'` and `pos_label=1`\n",
    ">- another thing, **Precision** and **Reacall** are more **effective** for Machine Learning than **F1**\n",
    ">- about ill-defined parameters, I found some documentation at [Udacity](https://knowledge.udacity.com/questions/314220)\n",
    "\n",
    "**Future improvement**\n",
    "\n",
    ">- there are better metrics for **multilabel classificication** [here](https://medium.com/analytics-vidhya/metrics-for-multi-label-classification-49cc5aeba1c3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjgxOWQxZTYxNDI5ZGQzZDNjYWVmMTI5YzBhYzJiYWU4YzZkNDZmYmMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MzAyNzYxNDYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTAzNjUxNTUwMDU1MTQ1OTkzNSIsImVtYWlsIjoiZXBhc3NldG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFZHVhcmRvIFBhc3NldG8iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2pJNmh5V3FSTGNfdHZCYlg4OWxFTEphZ3diMFBYeXJNOGN1YXBLR1E9czk2LWMiLCJnaXZlbl9uYW1lIjoiRWR1YXJkbyIsImZhbWlseV9uYW1lIjoiUGFzc2V0byIsImlhdCI6MTYzMDI3NjQ0NiwiZXhwIjoxNjMwMjgwMDQ2LCJqdGkiOiIzYzYyZThiZDhkYWU4YjU4NWJlZDI4ZGFhYjE5ZDkwY2MyOTFmNjhlIn0.kwd1YjjoxP-RUFHA86RftkGHMMwic3edRM31Yz8sJL9dg0jzPwS2c9peJ9kDuIQK5x8PWvZxhnl-wI32M_D_FvWv5UXad1cYnkuEGnxeo94LPCUam-aOnUvDDpefUEOv8Oe2751C0VH1MrlDiOQxyGcYBIjnr2NtdaN8Y8pm-ZLonqw3zpZO-2Wlkhnrb12ruZmpWD2CbqZCHpNwmYq0bQqCrNp_dCZ9mBjc5xrYN2G8Us7ESZcCnqLLjk_cb6UVV81LFjKkrjGifBsOac-ANoc7TBJQnFW41FISORWL8j84mW7jl8UgEmxrgc8kaFtHm6oC5ptc9YLRBDq1Q93ZBQ)\n",
    ">- we could use **Precision at k** `P@k`, **Avg precision at k** `AP@k`, **Mean avg precision at k** `MAP@k` and **Sampled F1 Score** `F1 Samples`\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** update: for **Naïve Bayes** updated new, **more realistic** metrics based on **10 top** labels:\n",
    "\n",
    ">- Model Accuracy now is **31.2**%\n",
    ">- Precision now is **85.9**%\n",
    ">- Recall now is **26.4**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** update: for **Naïve Bayes** letting the tokenizer take the same word more than once:\n",
    "\n",
    ">- Model Accuracy now is **31.5**%\n",
    ">- Precision now is **86.3**%\n",
    ">- Recall now is **26.6**%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_mbnb.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=True,\n",
    "                             verbose=True)\n",
    "#udacourse2.fn_scores_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if I want a **complete report**, over the 36 y-labels:\n",
    "\n",
    "- just set `best_10=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_mbnb.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=False,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy is distorted by **false fitting** (zeroes over zeroes)\n",
    "\n",
    "Manually, I could find the true meaning as near to **82%**\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** update: now this consideration is useless, as metrics were reformed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_f1 = [.78, .86, .83, .85, .80, .83, .81, .91, .86, .69, .83]\n",
    "\n",
    "corr_precision = statistics.mean(real_f1)\n",
    "print('F1 corrected Model Accuracy: {:.2f} ({:.0f}%)'.format(corr_precision, corr_precision*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critics about the performance of my Classifier\n",
    "\n",
    "I know what you are thinking: \"Uh, there is something **wrong** with the Accuracy of this guy\"\n",
    "\n",
    "So, as you can see: **92.2%** is too high for a **Naïve Bayes Classifier**!\n",
    "\n",
    "There are some explanations here:\n",
    "\n",
    ">- if you read it with care, you will find this **weird** label `related`. And it seems to **positivate** for every row on my dataset. So It distorts the average for a **higher** one\n",
    ">- if you look at each **weighted avg**, you will find some clearly **bad** values, as **68%** for **aid_related** (if you start thinking about it, is something like in **2/3** of the cases the model guesses well for this label... so a really **bad** performance)\n",
    "\n",
    "*Updated 1: when I removed `related` column, my **Model Accuracy** felt down to **56.1%**. Normally my Labels are holding something as **75-78%** f1-score. Now I think that these **untrainable columns** are making my average Accuracy to fall down!*\n",
    "\n",
    "---\n",
    "\n",
    "But there is another **critic** about this data.\n",
    "\n",
    "I am **Engineer** by profession. And I work for almost **19** years in a **hidrology** datacenter for the Brazillian Government. So, in some cases, you see some data and start thinking: \"this data is not what it seems\".\n",
    "\n",
    "And the main problem with this data is:\n",
    "\n",
    ">- it is a **mistake** to think that all we need to do with it is to train a **Supervised Learning** machine!\n",
    ">- if you look with care, this is not about **Supervised Learning**, it is an actual **Semi-Supervised Learning** problem. Why?\n",
    ">- just consider that there were **zillions** of Tweeter messages about catastrophes all around the world. And then, when the message was not originally in English, they translated it. And then someone manually **labeled** each of these catastrophe reports. And a **lot** of them remained with **no classification**\n",
    ">- it I just interpret it as a **Supervised Learning** challenge, I will feed my Classifier with a lot of **false negatives**. And my Machine Learning Model will learn how to **keep in blank** a lot of these messages, as it was trained by my **raw** data!\n",
    "\n",
    "So in **preprocessing** step, I avoided **unlabelled data**, filtering and removing for training every row that not contains any label on it. They were clearly, **negleted** for labeling, when manually processed!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Try other Classifiers\n",
    "\n",
    "- I will try some Classifiers based on a **hierarchical structure**:\n",
    "\n",
    ">- why **hierarchical structure** for words? Just because I think we do it **naturally** in our brain\n",
    ">- when science mimic nature I personally think that things goes in a better way. So, let´s try it!\n",
    "\n",
    "First of them, **Random Forest** Classifier\n",
    "\n",
    ">- as **RFC** is a **single-label** Classifier, we need to call it **n** times for each label to be classified\n",
    ">- so, que need to call it indirectly, using **Multi-Output** Classifier tool\n",
    ">- it took **693.73 seconds** (as 11 minutes and 35 seconds) to complete the tast (not so bad!)\n",
    ">- I tried to configure a **GridSearch**, just to set the number of processors to `-1` (meaning, the **maximum** number)\n",
    "\n",
    "Accuracy was near to **93%** before removing `related` label. Now it remains as **93.8%**. So, it don't matter!\n",
    "\n",
    "**Version 1.10** update: prepared other Machine Learning Classifiers for training the data\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** for **Random Forest** updated new, **more realistic** metrics based on **10 top** labels:\n",
    "\n",
    ">- Model Accuracy now is **66.5**%\n",
    ">- Precision now is **69.8**%\n",
    ">- Recall now is **70.1**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** for **Random Forest** letting the tokenizer take the same word more than once:\n",
    "\n",
    ">- Model Accuracy now is **66.4**%\n",
    ">- Precision now is **79.8**%\n",
    ">- Recall now is **59.7**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.19** for **Random Forest** :\n",
    "\n",
    ">- Model Accuracy now is **66.3**%\n",
    ">- Precision now is **79.5**%\n",
    ">- Recall now is **59.7**%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only uncomment if you really want use this code, it takes too much time to process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time()\n",
    "\n",
    "#def dummy(doc):\n",
    "#    return doc\n",
    "\n",
    "#Random Forest makes randomization, so random_state parameter was set\n",
    "#pipeline_rafo = Pipeline([('vect', CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=42)))])\n",
    "\n",
    "#pipeline_rafo.fit(X_train, y_train)\n",
    "\n",
    "#spent = time() - start\n",
    "#s_min = spent // 60\n",
    "#print('RANDOM FOREST - process time: {:.0f} minutes, {:.2f} seconds ({:.2f}s)'\\\n",
    "#      .format(s_min, spent-(s_min*60), spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_rafo.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=True,\n",
    "                             verbose=True)\n",
    "#udacourse2.fn_scores_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tree like Classifier is **Adaboost**:\n",
    "\n",
    ">- they say Adaboost is specially good for **differenciate** positives and negatives\n",
    ">- it took **106.16 seconds** (kind of **1** minute and **45** seconds) to complete the task... not so bad... (as AdaBoost don´t use **trees**, but **stumps** for doing its job)\n",
    "\n",
    "Accuracy was near to **91%**. After removing `related` label:\n",
    "\n",
    ">- it raised to **93.6%**. As Adaboost is based on **stumps**, a bad label perhaps distorts the model\n",
    ">- training time lowered to **71,57** seconds, so kind of a time reduction about 30%\n",
    "\n",
    "*Adaboost seems to be really **fast**, when compared to Random Forest. And without loosing too much in terms of Model Accuracy...*\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** for **Adaboost** updated new, **more realistic** metrics based on **10 top** labels:\n",
    "\n",
    ">- Model Accuracy now is **66.3**%\n",
    ">- Precision now is **77.7**%\n",
    ">- Recall now is **58.7**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** update: for **Adaboost** letting the tokenizer take the same word more than once:\n",
    "\n",
    ">- Model Accuracy now is **65.4**%\n",
    ">- Precision now is **77.3**%\n",
    ">- Recall now is **57.8**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.19** update: for **Adaboost** was not affected, as **Linear SVM** when I inserted two really problematic labels for training `related` (everything is labelled as **1**) and `missing_child` (everything is labelled as **0**)\n",
    "\n",
    ">- Model Accuracy now is **65.2**%\n",
    ">- Precision now is **77.5**%\n",
    ">- Recall now is **57.8**%\n",
    "\n",
    "**Version 1.20** update: after running **GridSearch** on Adaboost, I could make some ajustments on parameters:\n",
    "\n",
    ">- learning_rate $\\rightarrow$ was **1.0**, now is **0.5**\n",
    ">- n_estimators $\\rightarrow$ was **50**, now is **80**\n",
    "\n",
    "Train time was **100.84** seconds and now is **159.48** seconds\n",
    "\n",
    "And my model performance now is:\n",
    "\n",
    ">- Model Accuracy now is **64.0**%\n",
    ">- Precision now is **81.2**%\n",
    ">- Recall now is **55.1**%\n",
    "\n",
    "*So, with the new parameters **precision** increased near to 4%, but **recall** decreased near to 3%. Training time increased 60%. And I don´t think these new parameters are really nice*\n",
    "\n",
    "Another thing, I am trying `algorithm='SAMME'`. And why `SAMME`, just because we have a kind of **discrete** problem to solve, and this one is better for **discrete boosting**\n",
    "\n",
    ">- Model Accuracy now is **49.3**%\n",
    ">- Precision now is **80.6**%\n",
    ">- Recall now is **38.1**%\n",
    "\n",
    "*Not a good job, let´s keep the original algorithm!*\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.21** update: for preventing **pipeline leakage** using Picke, I modified `train_data` for having preprocessor as optional. For more details see reference[here](https://rebeccabilbro.github.io/module-main-has-no-attribute/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#CountVectorizer(tokenizer=udacourse2.fn_tokenize_fast)\n",
    "\n",
    "#Adaboost makes randomization, so random_state parameter was set\n",
    "pipeline_adab = Pipeline([('vect', CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf',  MultiOutputClassifier(AdaBoostClassifier(learning_rate=1.0,\n",
    "                                                                            n_estimators=50,\n",
    "                                                                            algorithm='SAMME.R',\n",
    "                                                                            random_state=42)))])\n",
    "pipeline_adab.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "print('ADABOOST - process time: {:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_adab.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=True,\n",
    "                             verbose=True)\n",
    "#udacourse2.fn_scores_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Falling in a trap when choosing another Classifier\n",
    "\n",
    "Then I tried a **Stochastic Gradient Descent** (SGD) [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "\n",
    "_\"Linear classifiers (SVM, logistic regression, etc.) with SGD training\"_\n",
    "\n",
    "It can works with a **Support Vector Machine** (SVM), that is a fancy way of defining a good frontier\n",
    "\n",
    "\n",
    "`clf = SGDClassifier()` with some parameters\n",
    "  \n",
    ">- `learning_rate='optimal'`$\\rightarrow$ **decreasing strength schedule** used for updating the gradient of the loss at each sample\n",
    ">- `loss='hinge'` $\\rightarrow$ **Linear SVM** for the fitting model (works with data represented as dense or sparse arrays for features)\n",
    ">- `penalty=[‘l2’, ‘l1’, ‘elasticnet’]` $\\rightarrow$ **regularizer**  shrinks model parameters towards the zero vector using an **Elastic Net** (l2) or \n",
    ">- `alpha=[1e-5, 1e-4, 1e-3]` $\\rightarrow$ stopping criteria, the higher the value, the **stronger** the regularization (also used to compute the **Learning Rate**, when set to learning_rate is set to ‘optimal’\n",
    ">- `n_iter=[1, 5, 10]` $\\rightarrow$ number of passes over the **Epochs** (Training Data). It only impacts the behavior in the **fit method**, and not the partial_fit method\n",
    ">- `random_state=42` $\\rightarrow$ if you want to replicate exactly the same output each time you retrain your machine\n",
    "  \n",
    "*Observe that this is a kind of a lecture over the text at SkLearn website for this Classifier*\n",
    "\n",
    "---\n",
    "\n",
    "And **SGDC** didn´t work! It gave me a **ValueError: y should be a 1d array, got an array instead**. So, something went wrong:\n",
    "\n",
    "Searching for the cause of the problem, I found this explanation [here](https://stackoverflow.com/questions/20335853/scikit-multilabel-classification-valueerror-bad-input-shape)\n",
    "\n",
    "*\"No, SGDClassifier does not do **multilabel classification** (what I need!) -- it does **multiclass classification**, which is a different problem, although both are solved using a one-vs-all problem reduction\"*\n",
    "\n",
    "*(we use Multiclass Classification when the possible classifications are **mutually exclusive**. For example, I have a picture with a kind of fruit, and it could be classified as a **banana**, or a **pear**, or even an **apple**. Clearly that is not our case!)*\n",
    "\n",
    "*Then, neither **SGD** nor OneVsRestClassifier.fit will accept a **sparse matrix** (is what I have!) for y* \n",
    "\n",
    "*- SGD wants an **array of labels** (is what I have!), as you've already found out*\n",
    "\n",
    "*- OneVsRestClassifier wants, for multilabel purposes, a list of lists of labels*\n",
    "\n",
    "*Observe that this is a kind of a lecture over the explanatory text that I got at SKLearn website for SGDC for Multilabel*\n",
    "\n",
    "---\n",
    "\n",
    "There is a good explanation about **Multiclass** and **Multilabel** Classifiers [here](https://scikit-learn.org/stable/modules/multiclass.html)\n",
    "\n",
    "Don´t try to run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time()\n",
    "\n",
    "#def dummy(doc):\n",
    "#    return doc\n",
    "\n",
    "#random_state=42 #<-just to remember!\n",
    "#pipeline_sgrd = Pipeline([('vect', CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('clf', SGDClassifier(loss='hinge', \n",
    "#                                                penalty='l2',\n",
    "#                                                alpha=1e-3))]) \n",
    "#fit_sgrd = pipeline_sgrd.fit(X_train, y_train)\n",
    "\n",
    "#spent = time() - start\n",
    "#print('STOCHASTIC GRADIENT DESCENT - process time:{:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try **K-Neighbors Classifier**\n",
    "\n",
    "**First** try, `n_neighbors=3`:\n",
    "\n",
    ">- model Accuracy was **91.8%**... not so bad!\n",
    ">- and... why only **3** neighbors? You see this parameter is quite **arbitrary** in our case... it could be 2 or 5... as we have so much (or so few neighbors that we can rely on, this can **tune better** our classifier)... and why not try it, using **GridSearch**?\n",
    "\n",
    "**Second** try, `n_neighbors=7` and `p=1` (using **GridSearch**, explanation below to tune it for a better result):\n",
    "\n",
    ">- it took **.74** seconds to **fit** the Classifier\n",
    ">- the slowest part was to **predict**, as **5** minutes and **27** seconds!\n",
    ">- it gave us **92.0%** of model Accuracy... and a lot of **non-fitting** labels!\n",
    ">- so, it was not a good idea to use the new parameters, the **original ones** are better!\n",
    "\n",
    "Some reflexions about models, **GridSearch** and best parameters:\n",
    "\n",
    ">- sometimes a **slight** difference don´t worth the computational price\n",
    ">- another thing to reflect about: why I started with only **3** neighbors? Just because Tweeter messages are quite **short**. When tokenized, the number of **tokens** normally don´t exceed **7**!\n",
    ">- so, giving a brutal **resolution** to poor data, normally is not a good idea\n",
    "\n",
    "**Third** try, `n_neighbors=3` and `p=1`\n",
    "\n",
    ">- I achieved **91.3** accuracy, don´t using so much computational power!\n",
    ">- only tunning a bit the **power** parameter provided me with a silghtly **better** result\n",
    ">- training time is **0.79** seconds and predict is **5** minutes and **27** seconds\n",
    "\n",
    "**Version 1.11** update: preparation of k-Neighbors Classifier for training\n",
    "\n",
    "*k-Neighbors seems to not fit so well for this kind of problems!*\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** for **k-Nearest** updated new, **more realistic** metrics based on **10 top** labels:\n",
    "\n",
    ">- Model Accuracy now is **39.1**%\n",
    ">- Precision now is **60.1**%\n",
    ">- Recall now is **32.6**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** for **k-Nearest** letting the tokenizer take the same word more than once:\n",
    "\n",
    ">- Model Accuracy now is **38.8**%\n",
    ">- Precision now is **60.5**%\n",
    ">- Recall now is **32.2**%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#k-Neighbors don´t use randomization\n",
    "pipeline_knbr = Pipeline([('vect', CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3, p=1)))])\n",
    "\n",
    "pipeline_knbr.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "print('K NEIGHBORS CLASSIFIER - process time: {:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "y_pred = pipeline_knbr.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=True,\n",
    "                             verbose=True)\n",
    "#udacourse2.fn_scores_report(y_test, y_pred)\n",
    "\n",
    "spent = time() - start\n",
    "print('process time: {:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Suport Vector Machine, fed by TfidVectorizer:\n",
    "    \n",
    ">- now, the idea is to train another type of machine, a **Support Vector Machine** (SVM)\n",
    ">- SVM uses another philosophy, as you create a coordinate space for **vectors**\n",
    ">- the space coordinate system can be a **cartesian planes**, or **polar combinations**\n",
    ">- the idea is to sepparate data using vectors as **sepparation elements**\n",
    ">- in this case, whe use only **linear** elements to make de sepparation\n",
    "\n",
    "Why **Linear**?\n",
    "\n",
    ">- the **computational cost** for linear entities on **discrete** computers is really low (if we were using **valved** computers, we could start exploring **non-linear** models with better profit)\n",
    ">- now we ned **fit** and **transform** opperations on our vectors provider\n",
    ">- it is a **fast** machine (**18.84**seconds), with the amazing Model Accuracy of a bit less than **93%** (one of the features could not be trained!)\n",
    ">- when corrected **labels consistencies**, based on our **hierarchical structure**, Model Accuracy raised a bit, reaching **93.6**!\n",
    "\n",
    "**Version 1.12** update: preparation of a completely different kind of **Machine Learning Classifier** (Support Vector Machine Family)\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** update: for **Linear Support Vector Machine** updated new, **more realistic** metrics based on **10 top** labels:\n",
    "\n",
    ">- Model Accuracy now is **70.6**%\n",
    ">- Precision now is **70.8**%\n",
    ">- Recall now is **71.1**%\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** update: for **Linear Support Vector Machine** letting the tokenizer take the same word more than once:\n",
    "\n",
    ">- Model Accuracy now is **70.5**%\n",
    ">- Precision now is **71.9**%\n",
    ">- Recall now is **69.7**%\n",
    "\n",
    "**Version 1.19** update: for Linear Support Vector Machine **deteriorate** a lot when I inserted two really problematic labels for training `related` (everything is labelled as **1**) and `missing_child` (everything is labelled as **0**)\n",
    "\n",
    "*I just re-inserted this two labels in order to accomplish one of the requisites for project approving at Udacity, that says to \"train all the 36 columns\". I am a bit angry about it, as it pushed down so much the performance of my project!*\n",
    "\n",
    ">- Model Accuracy now is **61.2**%\n",
    ">- Precision now is **80.4**%\n",
    ">- Recall now is **50.3**%\n",
    "\n",
    "**Version 1.19** update: I really **tried** to avoid both training warnings, just testing and elliminating **untrainable columns** from my labels. But jus to to follow the Udacity requisites for this project, I needed to deactivate these lines of code. So now we have these weird warnings:\n",
    "\n",
    "- `UserWarning: Label 0 is present in all training example` (this is for `related` column)\n",
    "\n",
    "- `UserWarning: Label not 9 is present in all training examples` (this is for `missing_child` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "feats = TfidfVectorizer(analyzer='word', \n",
    "                        tokenizer=dummy, \n",
    "                        preprocessor=dummy,\n",
    "                        token_pattern=None,\n",
    "                        ngram_range=(1, 3))\n",
    "\n",
    "classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                        random_state=42))\n",
    "\n",
    "#don´t use this line, I thought it was necessary to to te sepparation!\n",
    "#feats = feats.fit_transform(X_train)\n",
    "\n",
    "pipeline_lnsv = Pipeline([('vect', feats),\n",
    "                          ('clf', classif)])\n",
    "\n",
    "pipeline_lnsv.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "print('LINEAR SUPPORT VECTOR MACHINE - process time:{:.2f} seconds'.format(spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you experience:\n",
    "\n",
    "*NotFittedError: Vocabulary not fitted or provided*\n",
    "[here](https://stackoverflow.com/questions/60472925/python-scikit-svm-vocabulary-not-fitted-or-provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Test Area (for Version 1.16 improvement)\n",
    "\n",
    "I am trying to create new **fancy** metrics for scoring my Classifiers\n",
    "\n",
    ">- I was taking only the **General Average F1 Score** as metrics, and it seems so pooly detailed\n",
    "\n",
    "\n",
    "I have for most classified labels, according to my `fn_labels_report` function:\n",
    "\n",
    "1. related:19928 (75.9%)\n",
    "2. aid_related:10903 (41.5%)\n",
    "3. weather_related:7304 (27.8%)\n",
    "4. direct_report:5080 (19.4%)\n",
    "5. request:4480 (17.1%)\n",
    "6. other_aid:3448 (13.1%)\n",
    "7. food:2930 (11.2%)\n",
    "8. earthquake:2455 (9.4%)\n",
    "9. storm:2448 (9.3%)\n",
    "10. shelter:2319 (8.8%)\n",
    "11. floods:2158 (8.2%)\n",
    "\n",
    "When I remove **related** (as it will only classify as **\"1\"** for **All** my dataset, when I remove rows that have **no** classification at all - so, I cannot **train** on them), I will get these new columns as:\n",
    "\n",
    "1. aid_related\n",
    "2. weather_related\n",
    "3. direct_report\n",
    "4. request\n",
    "5. other_aid\n",
    "6. food\n",
    "7. earthquake\n",
    "8. storm\n",
    "9. shelter\n",
    "10. floods\n",
    "\n",
    "Turning them into a list:\n",
    "\n",
    "`top_labels = ['aid_related', 'weather_related', 'direct_report', 'request', 'other_aid', 'food', 'earthquake', 'storm', 'shelter', 'floods']`\n",
    "\n",
    "Retrieve their position by name [here](https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas):\n",
    "\n",
    "`y_test.columns.get_loc(\"offer\")`\n",
    "\n",
    "**Version 1.16** update: new `fn_scores_report2` function created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_lnsv.predict(X_test)\n",
    "udacourse2.fn_scores_report2(y_test, \n",
    "                             y_pred,\n",
    "                             best_10=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don´t use this function! (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = pipeline_lnsv.predict(X_test)\n",
    "#udacourse2.fn_scores_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Make a Fine Tunning effort over Classifiers\n",
    "\n",
    "#### First attempt: Stochastic Gradient Descent\n",
    "\n",
    "**Grid Search**\n",
    "\n",
    "`parameters = {'vect__ngram_range': [(1, 1), (1, 2)],`\n",
    "              `'tfidf__use_idf': (True, False),`\n",
    "              `'clf__alpha': (1e-2, 1e-3)}`\n",
    "\n",
    "- use **multiple cores** to process the task\n",
    "\n",
    "`gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)`\n",
    "\n",
    "`gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)`\n",
    "\n",
    "-see the **mean score** of the parameters\n",
    "\n",
    "`gs_clf.best_score_`\n",
    "\n",
    "`gs_clf.best_params_`\n",
    "\n",
    "*Not implemented, by the reason that our SGD effort was abandonned. Only some  sketches from my studies for GridSearch on SGD remain here! (source, SKlearn parameters + documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)*\n",
    "\n",
    "#### Second attempt: k-Neighbors\n",
    "\n",
    ">- we can see tunable parameters using the command `Class_k.get_params()`\n",
    ">- I tried to tune up for `n_neighbors` and for `p`\n",
    ">- it took **74** minutes and **15** seconds to run (so, don´t try it!)\n",
    ">- best estimator was **n_neighbors=7** and **p=1** $\\rightarrow$ \"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1)\" (from SkLearn documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1.13** update: implemented **Grid Search** for some sellected Classifiers\n",
    "\n",
    "**Future implementation**: test other parameters for a better fine-tunning (I don't made an **exaustive fine-tunning**!)\n",
    "\n",
    "Only uncomment if you really want use this code, it takes too much time to process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time()\n",
    "\n",
    "#def dummy(doc):\n",
    "#    return doc\n",
    "\n",
    "\n",
    "#k-Neighbors don´t use randomization\n",
    "#Vect_k = CountVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "#Transf_k = TfidfTransformer()\n",
    "#Class_k = MultiOutputClassifier(KNeighborsClassifier())\n",
    "\n",
    "#pipeline_knbr = Pipeline([('vect', Vect_k),\n",
    "#                          ('tfidf', Transf_k),\n",
    "#                          ('clf', Class_k)])\n",
    "\n",
    "#param_dict = {'clf__estimator__n_neighbors': [3,5,7],\n",
    "#              'clf__estimator__p': [1,2]}\n",
    "\n",
    "#estimator = GridSearchCV(estimator=pipeline_knbr, \n",
    "#                         param_grid=param_dict,\n",
    "#                         n_jobs=-1) #, scoring='roc_auc')\n",
    "\n",
    "#estimator.fit(X_train, y_train)\n",
    "\n",
    "#spent = time() - start\n",
    "#s_min = spent // 60\n",
    "#print('K NEIGHBORS GRID SEARCH - process time: {:.0f} minutes, {:.2f} seconds ({:.2f}s)'\\\n",
    "#      .format(s_min, spent-(s_min*60), spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_knbr.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1.10** update: Grid Search on Adaboost. As we choose this Classifier as the main classifier for our model, let´s make a **GridSearch** on it too:\n",
    "\n",
    ">-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time()\n",
    "\n",
    "#def dummy(doc):\n",
    "#    return doc\n",
    "\n",
    "#Adaboost makes randomization, so random_state parameter was set\n",
    "#vect_a = CountVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "#transf_a = TfidfTransformer()\n",
    "#class_a = MultiOutputClassifier(AdaBoostClassifier(random_state=42))\n",
    "\n",
    "#pipeline_adab = Pipeline([('vect', vect_a),\n",
    "#                          ('tfidf', transf_a),\n",
    "#                          ('clf', class_a)])\n",
    "\n",
    "#param_dict = {'clf__estimator__learning_rate': [0.5, 1.0],\n",
    "#              'clf__estimator__n_estimators': [50, 80]}\n",
    "\n",
    "\n",
    "#param_dict = {'clf__estimator__algorithm': ['SAMME.R', 'SAMME'],\n",
    "#              'clf__estimator__learning_rate': [0.5, 1.0, 2.0],\n",
    "#              'clf__estimator__n_estimators': [20, 50, 80]}\n",
    "\n",
    "#estimator = GridSearchCV(estimator=pipeline_adab, \n",
    "#                         param_grid=param_dict,\n",
    "#                         n_jobs=-1)\n",
    "\n",
    "#pipeline_adab.fit(X_train, y_train)\n",
    "#estimator.fit(X_train, y_train)\n",
    "\n",
    "#spent = time() - start\n",
    "#s_min = spent // 60\n",
    "#print('ADABOOST GRID SEARCH - process time: {:.0f} minutes, {:.2f} seconds ({:.2f}s)'\\\n",
    "#      .format(s_min, spent-(s_min*60), spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For estimator can could try: (Adaboost documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html))\n",
    "    \n",
    ">- 'estimator__base_estimator': None $\\rightarrow$ don´t change it, Adaboost is a **Decision Tree** with depth=1!\n",
    ">- 'estimator__algorithm': 'SAMME.R' $\\rightarrow$ 'SAMME' is **discrete boosting** (and for our problem, probably it will be better!)\n",
    ">- 'estimator__learning_rate': 1.0 $\\rightarrow$ n_estimators vs learning_rate... it is a **tradeoff**...\n",
    ">- 'estimator__n_estimators': 50 $\\rightarrow$ whe can play with **both**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Don´t run it, it´s only to get the parameters for Adaboost!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_a.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took **72**minutes and **56**seconds in my machine to run, and gave me as **best parameters**:\n",
    "\n",
    ">- learning_rate $\\rightarrow$ **0.5**\n",
    ">- n_estimators $\\rightarrow$ **80**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.best_estimator_[2].estimator.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.best_estimator_[2].estimator.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear SVC**: new parameter found by using **Grid Search**\n",
    "\n",
    "- `C=0.5`\n",
    "\n",
    "- run time for training the Classifier is **4**min **26**sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "feats = TfidfVectorizer(analyzer='word', \n",
    "                        tokenizer=dummy, \n",
    "                        preprocessor=dummy,\n",
    "                        token_pattern=None,\n",
    "                        ngram_range=(1, 3))\n",
    "classif = OneVsRestClassifier(LinearSVC())\n",
    "\n",
    "pipeline_lnsv = Pipeline([('vect', feats),\n",
    "                          ('clf', classif)])\n",
    "\n",
    "\n",
    "param_dict = {'clf__estimator__C': [0.1,0.5,1.0,2.0,5.0]}\n",
    "\n",
    "estimator = GridSearchCV(estimator=pipeline_lnsv, \n",
    "                         param_grid=param_dict,\n",
    "                         n_jobs=-1) #, scoring='roc_auc')\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "spent = time() - start\n",
    "s_min = spent // 60\n",
    "print('LINEAR SUPPORT VECTOR MACHINE GRID SEARCH - process time: {:.0f} minutes, {:.2f} seconds ({:.2f}s)'\\\n",
    "      .format(s_min, spent-(s_min*60), spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classif.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NotFittedError: Vocabulary not fitted or provided*\n",
    "[here](https://stackoverflow.com/questions/60472925/python-scikit-svm-vocabulary-not-fitted-or-provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Choosing my Classifier\n",
    "\n",
    "### Classifiers Training & Tunning Summary\n",
    "\n",
    "\n",
    "|      Classifier      | Model Accuracy | Time to Train |           Observation          |\n",
    "|:--------------------:|:--------------:|:-------------:|:------------------------------:|\n",
    "| Binomial Naive Bayes | less than 82%  | 0.68s         | 22 labels couldn't be trained! |\n",
    "| Random Forest        | less than 90%  | 11m 44s       | 3 labels couldn't be trained!  |\n",
    "| Adaboost             | 93.6%          | 100.5s        |                                |\n",
    "| k-Neighbors          | less than 90%  | 0.58s         | 3 labels couldn't be trained!  |\n",
    "| Linear SVM           | less than 93%  | 26.81s        | 2 labels couldn't be trained!  |\n",
    "\n",
    "*thanks for the service Tables Generator [here](https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "#### In my concept, the rank is\n",
    "\n",
    "**First** place, Adaboost. It seemed **reliable** and **fast** for this particular task, and it is a neat machine, really easy to understand\n",
    "\n",
    "**Second** place, Linear SVM. Some of these labels are really **hard** to train and it was really **fast**\n",
    "\n",
    "**Third** place, k-Neighbors. It is **fast** and seeme so realiable as **Random Forest**, that is **too hard** to train\n",
    "\n",
    "---\n",
    "\n",
    "And I will take... **Linear SVM**!\n",
    "\n",
    "*Why? just because I cannot **really** believe that some of these labels can be trained!*\n",
    "\n",
    "The \"bad guy\" were `tools`, `shops`, `aid centers` and the **real** problem involved is:\n",
    "\n",
    ">- `shops` $\\rightarrow$ 120\n",
    ">- `tools` $\\rightarrow$ 159\n",
    ">- `aid_centers` $\\rightarrow$ 309\n",
    "\n",
    ">- there are so **few** labelled rows for these 3 guys that I really cannot believe that any Machine Learning Classifier can really **train** for them!\n",
    ">- and what about **Adaboost**? Well, Adaboost is based on **stumps** algorithm. And by processing the data, it cannot really reach a true **zero**, as the stumps inside them do not allow this kind of thing. So, instead of a **1**, it will give you a **0.999%**, that worth nothing for practical uses\n",
    ">- lately I can run more **GridSearch** and over **Linear SVM**. Adaboost don't have so much options for future improvement\n",
    "\n",
    "So, I will use in my model **Linear SVM**\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.14** update: filtering for **valid** ones over critical labels\n",
    "\n",
    "Choosen model changed to **Adaboost**. Why?\n",
    "\n",
    ">- conting for **valid** labels showed that these labels are in fact **trainable**, but that is not so easy to do it\n",
    ">- probably they are pressed to **zero**, as there are much more **false negatives** under these labels\n",
    "\n",
    "**Future version** - as my labels columns are clearly **hierarchical**:\n",
    "\n",
    ">- I could break my original dataset into 3 **more specific** datasets, as `infrastructure_related`, `aid_related` and `weather_related`, and include in each one the rows that are **relevant**\n",
    ">- In this case, the noise caused by **false negatives** will decrease, turning easier for each training achieve a better score\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.17** updated: metrics **changed**, so my choice may change too!\n",
    "\n",
    "New table for **Classifier evaluation** (10 greatest labels):\n",
    "\n",
    "|      Classifier      | Precision | Recall | Worst Metrics |\n",
    "|:--------------------:|:---------:|:------:|:-------------:|\n",
    "| Binomial Naïve Bayes | 85.9%     | 26.4%  | 65.6% & 0.1%  |\n",
    "| Random Forest        | 79.8%     | 60.1%  | 62.2% & 8.4%  |\n",
    "| Adaboost             | 77.7%     | 58.7%  | 48.4% & 20.4% |\n",
    "| k-Neighbors          | 60.1%     | 32.6%  | 28.6% & 1.2%  |\n",
    "| Linear SVM           | 70.8%     | 71.1%  | 43.0% & 32.5% |\n",
    "\n",
    "*Random Forest is very **slow** to fit!*\n",
    "*k-Neighbors is really **slow** to predict!*\n",
    "\n",
    "So, now I can see a lot of advantage for choosing **Linear SVM**:\n",
    "\n",
    ">- it is not **slow** for fit/train\n",
    ">- I can later explorer other better parameters using **GridSearch**\n",
    ">- It **don´t decay** so fast, for labels without so much rows for train\n",
    "\n",
    "My second choice is **Adaboost**\n",
    "\n",
    "*If things don´t go pretty well, I have a fancy alternative!*\n",
    "\n",
    "**Version 1.18**: letting the tokenizer take the same word more than once:\n",
    "\n",
    "|      Classifier      | Precision | Recall | Worst Metrics | Observations                  |\n",
    "|:--------------------:|:---------:|:------:|:-------------:|:-----------------------------:|\n",
    "| Binomial Naïve Bayes | 86.3%     | 26.6%  | 64.5% & 0.1%  | Imperceptible changes         |\n",
    "| Random Forest        | 79.8%     | 59.7%  | 61.8% & 9.3%  | Recall lowered a bit          |\n",
    "| Adaboost             | 77.3%     | 55.8%  | 46.1% & 15.9% | Recall lowered a bit          |\n",
    "| k-Neighbors          | 60.5%     | 32.2%  | 29.5% & 1.9%  | Parameters slightly increased |\n",
    "| Linear SVM           | 70.5%     | 71.9%  | 44.7% & 35.8% | Parameters slightly increased |\n",
    "\n",
    "*Fo, I will **keep** my tokenizer letting repeated tokens for each message, as I choose to use **Linear SVM**. If in future, training will turn so slow (as I get more and more messages at my dataset for training), I can go back to the earlier setting (only unique tokens per message)*\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.19** update: for **Linear SVM** when I inserted two really problematic labels for training `related` (everything is labelled as **1**) and `missing_child` (everything is labelled as **0**)\n",
    "\n",
    "*I only made this re-insertion for accomplishing the requisites for Udacity project aproval, as they really degradated the training of a SVM. And SVMs are really powerful Classifiers, so it was a pity to lost it!*\n",
    "\n",
    "*now, my project has as **main** (default) classifier back to **Adaboost**. The LSVM remains in my function, but for using it, you need to use a special parameter. The documentation how to use it is at `train_classifier.py`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the amount of **positive** data for **few** data on the labels:\n",
    "\n",
    "- observe that `child_alone` was previously removed from our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.columns[5:]]\n",
    "a = df2.apply(pd.Series.value_counts).loc[1]\n",
    "a[a < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean score of the parameters\n",
    "#gs_clf.best_score_\n",
    "#gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose your model, with the fine tunning already done (this can be changed later!)\n",
    "\n",
    "How to deal with picke [here](https://www.codegrepper.com/code-examples/python/save+and+load+python+pickle+stackoverflow)\n",
    "\n",
    "Pickle documentation [here](https://docs.python.org/3/library/pickle.html#module-pickle)\n",
    "\n",
    "2. Final considerations about this model:\n",
    "\n",
    ">- I choosed **Adaboost** as our Classifier\n",
    ">- The explanation for my choice is at the item **above**\n",
    "\n",
    "---\n",
    "\n",
    "**Version 1.18** update: now my Classifier was changed to **Linear SVC**. The explanations for my choice rests **above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the **Demo** code, that I found at **Codegreeper.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dic = {'hello': 'world'}\n",
    "\n",
    "with open('filename.pkl', 'wb') as pk_writer: #wb is for write+binary\n",
    "    pickle.dump(dic, \n",
    "                pk_writer, \n",
    "                protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('filename.pkl', 'rb') as pk_reader: #rb is for read+binary\n",
    "    dic_unpk = pickle.load(pk_reader)\n",
    "\n",
    "print (dic == dic_unpk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'classifier.pkl'\n",
    "\n",
    "with open (file_name, 'wb') as pk_writer: \n",
    "    pickle.dump(pipeline_lnsv, pk_writer)\n",
    "    \n",
    "with open('classifier.pkl', 'rb') as pk_reader: #rb is for read+binary\n",
    "    pipeline_lnsv = pickle.load(pk_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lnsv.fit(X_train, y_train)\n",
    "pipeline_lnsv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Use the notebook to complete `train.py`\n",
    "\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('under development')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import udacourse2 #my library for this project!\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#SQLAlchemy toolkit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import pool\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "#Machine Learning preparing/preprocessing toolkits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Machine Learning Feature Extraction tools\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Machine Learning Classifiers\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Machine Learning Classifiers extra tools\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pickling tool\n",
    "import pickle\n",
    "\n",
    "#only a dummy function, as I pre-tokenize my data\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def load_data(data_file, \n",
    "              verbose=False):\n",
    "    '''This function takes a path for a MySQL table and returns processed data\n",
    "    for training a Machine Learning Classifier\n",
    "    Inputs:\n",
    "      - data_file (mandatory) - full path for SQLite table - text string\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Outputs:\n",
    "      - X - tokenized text X-training - Pandas Series\n",
    "      - y - y-multilabels 0|1 - Pandas Dataframe'''\n",
    "    if verbose:\n",
    "        print('###load_data function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.read in file\n",
    "    #importing MySQL to Pandas - load data from database\n",
    "    engine = create_engine(data_file, poolclass=pool.NullPool) #, echo=True)\n",
    "    #retrieving tables names from my DB\n",
    "    inspector = inspect(engine)\n",
    "    if verbose:\n",
    "        print('existing tables in my SQLite database:', inspector.get_table_names())\n",
    "    connection = engine.connect()\n",
    "    df = pd.read_sql('SELECT * FROM Messages', con=connection)\n",
    "    connection.close()\n",
    "    df.name = 'df'\n",
    "    \n",
    "    #2.clean data\n",
    "    #2.1.Elliminate rows with all-blank labels\n",
    "    if verbose:\n",
    "        print('all labels are blank in {} rows'.format(df[df['if_blank'] == 1].shape[0]))\n",
    "    df = df[df['if_blank'] == 0]\n",
    "    if verbose:\n",
    "        print('remaining rows:', df.shape[0])\n",
    "    #Verifying if removal was complete\n",
    "    if df[df['if_blank'] == 1].shape[0] == 0:\n",
    "        if verbose:\n",
    "            print('removal complete!')\n",
    "        else:\n",
    "            raise Exception('something went wrong with rows removal before training')\n",
    "            \n",
    "    #2.2.Premature Tokenization Strategy (pre-tokenizer)\n",
    "    #Pre-Tokenizer + not removing provisory tokenized column\n",
    "    #inserting a tokenized column\n",
    "    try:\n",
    "        df = df.drop('tokenized', axis=1)\n",
    "    except KeyError:\n",
    "        print('OK')\n",
    "    df.insert(1, 'tokenized', np.nan)\n",
    "    #tokenizing over the provisory\n",
    "    df['tokenized'] = df.apply(lambda x: udacourse2.fn_tokenize_fast(x['message']), axis=1)\n",
    "    #removing NaN over provisory (if istill exist)\n",
    "    df = df[df['tokenized'].notnull()]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('found {} rows with no tokens'.format(empty_tokens))\n",
    "    df = df[df['tokenized'].apply(lambda x: len(x)) > 0]\n",
    "    empty_tokens = df[df['tokenized'].apply(lambda x: len(x)) == 0].shape[0]\n",
    "    if verbose:\n",
    "        print('*after removal, found {} rows with no tokens'.format(empty_tokens))\n",
    "    #I will drop the original 'message' column\n",
    "    try:\n",
    "        df = df.drop('message', axis=1)\n",
    "    except KeyError:\n",
    "        if verbose:\n",
    "            print('OK')\n",
    "    if verbose:\n",
    "        print('now I have {} rows to train'.format(df.shape[0]))\n",
    "\n",
    "    #2.3.Database Data Consistency Check/Fix\n",
    "    #correction for aid_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='aid',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for weather_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='wtr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for infrastrucutre_related\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='ifr',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    #correction for related(considering that the earlier were already corrected)\n",
    "    df = udacourse2.fn_group_check(dataset=df,\n",
    "                                   subset='main',\n",
    "                                   correct=True, \n",
    "                                   shrink=False, \n",
    "                                   shorten=False, \n",
    "                                   verbose=True)\n",
    "    \n",
    "    #load to database <-I don't know for what it is\n",
    "    \n",
    "    #3.Define features and label arrays (break the data)\n",
    "    #3.1.X is the Training Text Column\n",
    "    X = df['tokenized']\n",
    "    #3.2.y is the Classification labels\n",
    "    #I REMOVED \"related\" column from my labels, as it is impossible to train it!\n",
    "    y = df[df.columns[4:]]\n",
    "    #y = df[df.columns[5:]]\n",
    "    #remove_lst = []\n",
    "\n",
    "    #for column in y.columns:\n",
    "    #    col = y[column]\n",
    "    #    if (col == 0).all():\n",
    "    #        if verbose:\n",
    "    #            print('*{} -> only zeroes training column!'.format(column))\n",
    "    #        remove_lst.append(column)\n",
    "    #    else:\n",
    "            #print('*{} -> column OK'.format(column))\n",
    "    #        pass\n",
    "        \n",
    "    #if verbose:\n",
    "    #    print(remove_lst)\n",
    "    #y = y.drop(remove_lst, axis=1)\n",
    "    \n",
    "    spent = time() - start\n",
    "    if y.shape[1] == 36:\n",
    "        if verbose:\n",
    "            print('y dataset has 36 labels')\n",
    "            print('*dataset breaked into X-Training Text Column and Y-Multilabels')    \n",
    "            print('process time:{:.0f} seconds'.format(spent))\n",
    "    else:\n",
    "        raise Exception('something went wrong, dataset has {} labels instead of 36'.format(y.shape[1]))\n",
    "    return X, y\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def build_model(verbose=False):\n",
    "    '''This function builds the Classifier Pipeline, for future fitting\n",
    "    Inputs:\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - model_pipeline for your Classifiear (untrained)\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###build_model function started')\n",
    "    start = time()\n",
    "    \n",
    "    #1.text processing and model pipeline\n",
    "    #(text processing was made at a earlier step, at Load Data function)\n",
    "    feats = TfidfVectorizer(analyzer='word', \n",
    "                            tokenizer=dummy, \n",
    "                            preprocessor=dummy,\n",
    "                            token_pattern=None,\n",
    "                            ngram_range=(1, 3))\n",
    "    \n",
    "    classif = OneVsRestClassifier(LinearSVC(C=2., \n",
    "                                            random_state=42))\n",
    "    \n",
    "    model_pipeline = Pipeline([('vect', feats),\n",
    "                               ('clf', classif)])\n",
    "    \n",
    "    #define parameters for GridSearchCV (parameters already defined)\n",
    "    #create gridsearch object and return as final model pipeline (made at pipeline preparation)\n",
    "    #obs: for better performance, I pre-tokenized my data. And GridSearch was runned on Jupyter,\n",
    "    #     and the best parameters where adjusted, just to save processing time during code execution.\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*Linear Support Vector Machine pipeline was created')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model_pipeline\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def train(X, \n",
    "          y, \n",
    "          model, \n",
    "          verbose=False):\n",
    "    '''This function trains your already created Classifier Pipeline\n",
    "    Inputs:\n",
    "      - X (mandatory) - tokenized data for training - Pandas Series\n",
    "      - y (mandatory) - Multilabels 0|1 - Pandas Dataset\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output:\n",
    "      - trained model'''\n",
    "    if verbose:\n",
    "        print('###train function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Train test split\n",
    "    #Split makes randomization, so random_state parameter was set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.25, \n",
    "                                                        random_state=42)\n",
    "    if (X_train.shape[0] + X_test.shape[0]) == X.shape[0]:\n",
    "        if verbose:\n",
    "            print('data split into train and text seems OK')\n",
    "    else:\n",
    "        raise Exception('something went wrong when splitting the data')\n",
    "        \n",
    "    #2.fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # output model test results\n",
    "    y_pred = model.predict(X_test)\n",
    "    if verbose:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=True)\n",
    "    else:\n",
    "        metrics = udacourse2.fn_scores_report2(y_test, \n",
    "                                               y_pred,\n",
    "                                               best_10=True,\n",
    "                                               verbose=False)\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric < 0.6:\n",
    "            raise Exception('something is wrong, model is predicting poorly')\n",
    "\n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*classifier was trained!')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return model\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def export_model(model,\n",
    "                 file_name='classifier.pkl',\n",
    "                 verbose=False):\n",
    "    '''This function writes your already trained Classifiear as a Picke Binary\n",
    "    file.\n",
    "    Inputs:\n",
    "      - model (mandatory) - your already trained Classifiear - Python Object\n",
    "      - file_name (optional) - the name of the file to be created (default:\n",
    "         'classifier.pkl')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "       Output: return True if everything runs OK\n",
    "      ''' \n",
    "    if verbose:\n",
    "        print('###export_model function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Export model as a pickle file\n",
    "    file_name = file_name\n",
    "\n",
    "    #writing the file\n",
    "    with open (file_name, 'wb') as pk_writer: \n",
    "        pickle.dump(model, pk_writer)\n",
    "\n",
    "    #reading the file\n",
    "    #with open('classifier.pkl', 'rb') as pk_reader:\n",
    "    #    model = pickle.load(pk_reader)\n",
    "    \n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('*trained Classifier was exported')\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "        \n",
    "    return True\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def run_pipeline(data_file='sqlite:///Messages.db', \n",
    "                 verbose=False):\n",
    "    '''This function is a caller: it calls load, build, train and save modules\n",
    "    Inputs:\n",
    "      - data_file (optional) - complete path to the SQLite datafile to be \n",
    "        processed - (default='sqlite:///Messages.db')\n",
    "      - verbose (optional) - if you want some verbosity during the running \n",
    "        (default=False)\n",
    "    Output: return True if everything runs OK\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###run_pipeline function started')\n",
    "    start = time()\n",
    "\n",
    "    #1.Run ETL pipeline\n",
    "    X, y = load_data(data_file, \n",
    "                     verbose=verbose)\n",
    "    #2.Build model pipeline\n",
    "    model = build_model(verbose=verbose)\n",
    "    #3.Train model pipeline\n",
    "    model = train(X, \n",
    "                  y, \n",
    "                  model, \n",
    "                  verbose=verbose)\n",
    "    # save the model\n",
    "    export_model(model,\n",
    "                 verbose=verbose)\n",
    "    \n",
    "    spent = time() - start\n",
    "    if verbose:\n",
    "        print('process time:{:.0f} seconds'.format(spent))\n",
    "    return True\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def main():\n",
    "    '''This is the main Machine Learning Pipeline function. It calls the other \n",
    "    ones, in the correctorder.\n",
    "    '''\n",
    "    data_file = sys.argv[1]  # get filename of dataset\n",
    "    run_pipeline(data_file='sqlite:///Messages.db',\n",
    "                 verbose=True)\n",
    "\n",
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P@k` implementation [here](https://medium.com/analytics-vidhya/metrics-for-multi-label-classification-49cc5aeba1c3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjgxOWQxZTYxNDI5ZGQzZDNjYWVmMTI5YzBhYzJiYWU4YzZkNDZmYmMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MzAyNzYxNDYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTAzNjUxNTUwMDU1MTQ1OTkzNSIsImVtYWlsIjoiZXBhc3NldG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFZHVhcmRvIFBhc3NldG8iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2pJNmh5V3FSTGNfdHZCYlg4OWxFTEphZ3diMFBYeXJNOGN1YXBLR1E9czk2LWMiLCJnaXZlbl9uYW1lIjoiRWR1YXJkbyIsImZhbWlseV9uYW1lIjoiUGFzc2V0byIsImlhdCI6MTYzMDI3NjQ0NiwiZXhwIjoxNjMwMjgwMDQ2LCJqdGkiOiIzYzYyZThiZDhkYWU4YjU4NWJlZDI4ZGFhYjE5ZDkwY2MyOTFmNjhlIn0.kwd1YjjoxP-RUFHA86RftkGHMMwic3edRM31Yz8sJL9dg0jzPwS2c9peJ9kDuIQK5x8PWvZxhnl-wI32M_D_FvWv5UXad1cYnkuEGnxeo94LPCUam-aOnUvDDpefUEOv8Oe2751C0VH1MrlDiOQxyGcYBIjnr2NtdaN8Y8pm-ZLonqw3zpZO-2Wlkhnrb12ruZmpWD2CbqZCHpNwmYq0bQqCrNp_dCZ9mBjc5xrYN2G8Us7ESZcCnqLLjk_cb6UVV81LFjKkrjGifBsOac-ANoc7TBJQnFW41FISORWL8j84mW7jl8UgEmxrgc8kaFtHm6oC5ptc9YLRBDq1Q93ZBQ)\n",
    "\n",
    "\"Given a list of actual classes and predicted classes, precision at k would be defined as the number of correct predictions considering only the top k elements of each class divided by k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patk(actual, pred, k):\n",
    "\t#we return 0 if k is 0 because \n",
    "\t#   we can't divide the no of common values by 0 \n",
    "\tif k == 0:\n",
    "\t\treturn 0\n",
    "\n",
    "\t#taking only the top k predictions in a class \n",
    "\tk_pred = pred[:k]\n",
    "\n",
    "\t#taking the set of the actual values \n",
    "\tactual_set = set(actual)\n",
    "\n",
    "\t#taking the set of the predicted values \n",
    "\tpred_set = set(k_pred)\n",
    "\n",
    "\t#taking the intersection of the actual set and the pred set\n",
    "\t\t# to find the common values\n",
    "\tcommon_values = actual_set.intersection(pred_set)\n",
    "\n",
    "\treturn len(common_values)/len(pred[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the values of the actual and the predicted class\n",
    "y_true = [1 ,2, 0]\n",
    "y_pred = [1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(patk(y_true, y_pred,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AP@k` implementation [here](https://medium.com/analytics-vidhya/metrics-for-multi-label-classification-49cc5aeba1c3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjgxOWQxZTYxNDI5ZGQzZDNjYWVmMTI5YzBhYzJiYWU4YzZkNDZmYmMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MzAyNzYxNDYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTAzNjUxNTUwMDU1MTQ1OTkzNSIsImVtYWlsIjoiZXBhc3NldG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFZHVhcmRvIFBhc3NldG8iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2pJNmh5V3FSTGNfdHZCYlg4OWxFTEphZ3diMFBYeXJNOGN1YXBLR1E9czk2LWMiLCJnaXZlbl9uYW1lIjoiRWR1YXJkbyIsImZhbWlseV9uYW1lIjoiUGFzc2V0byIsImlhdCI6MTYzMDI3NjQ0NiwiZXhwIjoxNjMwMjgwMDQ2LCJqdGkiOiIzYzYyZThiZDhkYWU4YjU4NWJlZDI4ZGFhYjE5ZDkwY2MyOTFmNjhlIn0.kwd1YjjoxP-RUFHA86RftkGHMMwic3edRM31Yz8sJL9dg0jzPwS2c9peJ9kDuIQK5x8PWvZxhnl-wI32M_D_FvWv5UXad1cYnkuEGnxeo94LPCUam-aOnUvDDpefUEOv8Oe2751C0VH1MrlDiOQxyGcYBIjnr2NtdaN8Y8pm-ZLonqw3zpZO-2Wlkhnrb12ruZmpWD2CbqZCHpNwmYq0bQqCrNp_dCZ9mBjc5xrYN2G8Us7ESZcCnqLLjk_cb6UVV81LFjKkrjGifBsOac-ANoc7TBJQnFW41FISORWL8j84mW7jl8UgEmxrgc8kaFtHm6oC5ptc9YLRBDq1Q93ZBQ)\n",
    "\n",
    "\"It is defined as the average of all the precision at k for k =1 to k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apatk(acutal, pred, k):\n",
    "\t#creating a list for storing the values of precision for each k \n",
    "\tprecision_ = []\n",
    "\tfor i in range(1, k+1):\n",
    "\t\t#calculating the precision at different values of k \n",
    "\t\t#      and appending them to the list \n",
    "\t\tprecision_.append(pk.patk(acutal, pred, i))\n",
    "\n",
    "\t#return 0 if there are no values in the list\n",
    "\tif len(precision_) == 0:\n",
    "\t\treturn 0 \n",
    "\n",
    "\t#returning the average of all the precision values\n",
    "\treturn np.mean(precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the values of the actual and the predicted class\n",
    "y_true = [[1,2,0,1], [0,4], [3], [1,2]]\n",
    "y_pred = [[1,1,0,1], [1,4], [2], [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tfor i in range(len(y_true)):\n",
    "\t\tfor j in range(1, 4):\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"\"\"\n",
    "\t\t\t\ty_true = {y_true[i]}\n",
    "\t\t\t\ty_pred = {y_pred[i]}\n",
    "\t\t\t\tAP@{j} = {apatk(y_true[i], y_pred[i], k=j)}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MAP@k` implementation [here](https://medium.com/analytics-vidhya/metrics-for-multi-label-classification-49cc5aeba1c3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjgxOWQxZTYxNDI5ZGQzZDNjYWVmMTI5YzBhYzJiYWU4YzZkNDZmYmMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MzAyNzYxNDYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTAzNjUxNTUwMDU1MTQ1OTkzNSIsImVtYWlsIjoiZXBhc3NldG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFZHVhcmRvIFBhc3NldG8iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2pJNmh5V3FSTGNfdHZCYlg4OWxFTEphZ3diMFBYeXJNOGN1YXBLR1E9czk2LWMiLCJnaXZlbl9uYW1lIjoiRWR1YXJkbyIsImZhbWlseV9uYW1lIjoiUGFzc2V0byIsImlhdCI6MTYzMDI3NjQ0NiwiZXhwIjoxNjMwMjgwMDQ2LCJqdGkiOiIzYzYyZThiZDhkYWU4YjU4NWJlZDI4ZGFhYjE5ZDkwY2MyOTFmNjhlIn0.kwd1YjjoxP-RUFHA86RftkGHMMwic3edRM31Yz8sJL9dg0jzPwS2c9peJ9kDuIQK5x8PWvZxhnl-wI32M_D_FvWv5UXad1cYnkuEGnxeo94LPCUam-aOnUvDDpefUEOv8Oe2751C0VH1MrlDiOQxyGcYBIjnr2NtdaN8Y8pm-ZLonqw3zpZO-2Wlkhnrb12ruZmpWD2CbqZCHpNwmYq0bQqCrNp_dCZ9mBjc5xrYN2G8Us7ESZcCnqLLjk_cb6UVV81LFjKkrjGifBsOac-ANoc7TBJQnFW41FISORWL8j84mW7jl8UgEmxrgc8kaFtHm6oC5ptc9YLRBDq1Q93ZBQ)\n",
    "\n",
    "\"The average of all the values of `AP@k` over the whole training data is known as `MAP@k`. This helps us give an accurate representation of the accuracy of whole prediction data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import apk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(acutal, pred, k):\n",
    "\n",
    "\t#creating a list for storing the Average Precision Values\n",
    "\taverage_precision = []\n",
    "\t#interating through the whole data and calculating the apk for each \n",
    "\tfor i in range(len(acutal)):\n",
    "\t\taverage_precision.append(apk.apatk(acutal[i], pred[i], k))\n",
    "\n",
    "\t#returning the mean of all the data\n",
    "\treturn np.mean(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the values of the actual and the predicted class\n",
    "y_true = [[1,2,0,1], [0,4], [3], [1,2]]\n",
    "y_pred = [[1,1,0,1], [1,4], [2], [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(mapk(y_true, y_pred,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`F1 Samples` implementation [here](https://medium.com/analytics-vidhya/metrics-for-multi-label-classification-49cc5aeba1c3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjgxOWQxZTYxNDI5ZGQzZDNjYWVmMTI5YzBhYzJiYWU4YzZkNDZmYmMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MzAyNzYxNDYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTAzNjUxNTUwMDU1MTQ1OTkzNSIsImVtYWlsIjoiZXBhc3NldG9AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFZHVhcmRvIFBhc3NldG8iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2pJNmh5V3FSTGNfdHZCYlg4OWxFTEphZ3diMFBYeXJNOGN1YXBLR1E9czk2LWMiLCJnaXZlbl9uYW1lIjoiRWR1YXJkbyIsImZhbWlseV9uYW1lIjoiUGFzc2V0byIsImlhdCI6MTYzMDI3NjQ0NiwiZXhwIjoxNjMwMjgwMDQ2LCJqdGkiOiIzYzYyZThiZDhkYWU4YjU4NWJlZDI4ZGFhYjE5ZDkwY2MyOTFmNjhlIn0.kwd1YjjoxP-RUFHA86RftkGHMMwic3edRM31Yz8sJL9dg0jzPwS2c9peJ9kDuIQK5x8PWvZxhnl-wI32M_D_FvWv5UXad1cYnkuEGnxeo94LPCUam-aOnUvDDpefUEOv8Oe2751C0VH1MrlDiOQxyGcYBIjnr2NtdaN8Y8pm-ZLonqw3zpZO-2Wlkhnrb12ruZmpWD2CbqZCHpNwmYq0bQqCrNp_dCZ9mBjc5xrYN2G8Us7ESZcCnqLLjk_cb6UVV81LFjKkrjGifBsOac-ANoc7TBJQnFW41FISORWL8j84mW7jl8UgEmxrgc8kaFtHm6oC5ptc9YLRBDq1Q93ZBQ)\n",
    "\n",
    "\"This metric calculates the F1 score for each instance in the data and then calculates the average of the F1 scores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_sampled(actual, pred):\n",
    "    #converting the multi-label classification to a binary output\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    actual = mlb.fit_transform(actual)\n",
    "    pred = mlb.fit_transform(pred)\n",
    "\n",
    "    #fitting the data for calculating the f1 score \n",
    "    f1 = f1_score(actual, pred, average = \"samples\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the values of the actual and the predicted class\n",
    "y_true = [[1,2,0,1], [0,4], [3], [1,2]]\n",
    "y_pred = [[1,1,0,1], [1,4], [2], [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f1_sampled(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
